{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T22:37:47.535714Z",
     "start_time": "2019-11-26T22:37:44.799814Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from utils import *\n",
    "# data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# image processing\n",
    "from PIL import Image\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "# tf and keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, concatenate, Conv2D, MaxPooling2D, Conv2DTranspose, Dropout, BatchNormalization\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K\n",
    "\n",
    "# dataset processing, ml models and metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import scipy\n",
    "import glob\n",
    "# import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T22:37:47.541724Z",
     "start_time": "2019-11-26T22:37:47.538707Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_ = 'HASYv2/'\n",
    "model_dir = 'trained_models/'\n",
    "data_dir = 'data/'\n",
    "processed_data_dir ='processed_data/'\n",
    "K.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\"\"\"\n",
    "# CNN without Batch Normalization\n",
    "def get_layers(input_shape, data_format, classes):\n",
    "    return [\n",
    "        Conv2D(25, (5, 5), padding='same', data_format=data_format, input_shape=input_shape),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        \n",
    "        Conv2D(50, (3, 3), padding='same', data_format=data_format),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "                \n",
    "        Conv2D(100, (2,2), padding='same', data_format=data_format),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        \n",
    "        Conv2D(200, (2,2), padding='same', data_format=data_format),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(3200),\n",
    "        Activation('relu'),\n",
    "        \n",
    "        Dense(1600),\n",
    "        Activation('relu'),\n",
    "        \n",
    "        Dense(classes),\n",
    "        Activation('softmax'),\n",
    "    ]\n",
    "\n",
    "def create_network_1(input_shape, data_format, classes):\n",
    "    model = Sequential()\n",
    "    layers = get_layers(input_shape, data_format, classes)\n",
    "    for i in layers:\n",
    "        model.add(i)\n",
    "    return model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN with Batch Normalization\n",
    "def get_layers_bn(input_shape, data_format, classes):\n",
    "    return [\n",
    "        Conv2D(25, (5, 5), padding='same', data_format=data_format, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        \n",
    "        Conv2D(50, (3, 3), padding='same', data_format=data_format),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "                \n",
    "        Conv2D(100, (2,2), padding='same', data_format=data_format),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        \n",
    "        Conv2D(200, (2,2), padding='same', data_format=data_format),\n",
    "        BatchNormalization(),\n",
    "        Activation('relu'),\n",
    "        MaxPooling2D(pool_size=(2,2), strides=(2,2)),\n",
    "        \n",
    "        Flatten(),\n",
    "        Dense(3200),\n",
    "        Activation('relu'),\n",
    "        \n",
    "        Dense(1600),\n",
    "        Activation('relu'),\n",
    "        \n",
    "        Dense(classes),\n",
    "        Activation('softmax'),\n",
    "    ]\n",
    "\n",
    "def create_network_2(input_shape, data_format, classes):\n",
    "    model = Sequential()\n",
    "    layers = get_layers_bn(input_shape, data_format, classes)\n",
    "    for i in layers:\n",
    "        model.add(i)\n",
    "    return model                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_test(test_x, test_y, models, sym2latex_dict):\n",
    "    if len(models) < 0:\n",
    "        raise ValueError(\"No models found in models variable\")\n",
    "    if len(test_x) != len(test_y):\n",
    "        raise ValueError(\"Varibales test_x and test_y are of different length\")\n",
    "    \n",
    "    # convert test values from one hot encoded to label value\n",
    "    y_true = []\n",
    "    for i in range(len(test_y)):\n",
    "        # print(one_hot_encode_to_char(res[i], threshold = 0.5, get_max=False))\n",
    "        val = one_hot_encode_to_char(test_y[i], threshold = 0.9, get_max=True)\n",
    "        if len(val) > 0:\n",
    "            y_true.append(val[0])\n",
    "        else:\n",
    "            y_true.append(None)\n",
    "            print(\":(\")\n",
    "    no_models = len(models)\n",
    "    cols = ['symbol','latex']\n",
    "    preds = []\n",
    "    # convert predictions from one hot encoded to label value\n",
    "    for i in range(no_models):\n",
    "        print(\"Predictng labels for Model \"+str(i))\n",
    "        cols.append('model'+str(i))\n",
    "        \n",
    "        res = models[i].predict(X_test)\n",
    "        y_pred = []\n",
    "        for j in range(len(res)):\n",
    "            # print(one_hot_encode_to_char(res[i], threshold = 0.5, get_max=False))\n",
    "            val = one_hot_encode_to_char(res[j], threshold = 0.1, get_max=True)\n",
    "            if len(val) > 0:\n",
    "                y_pred.append(val[0])\n",
    "            else:\n",
    "                y_pred.append(None)\n",
    "                print(\":(\")\n",
    "        preds.append(y_pred)\n",
    "    print(\"Predictions Done\")\n",
    "    print(\"Comparing Results...\")\n",
    "    \n",
    "    # perform t test\n",
    "    t_test_result = pd.DataFrame(columns=cols)\n",
    "    for i in range(len(y_true)):\n",
    "        t_test_result.loc[i,'symbol'] = y_true[i]\n",
    "        t_test_result.loc[i,'latex'] = sym2latex_dict[y_true[i]]\n",
    "        for j in range(no_models):\n",
    "            t_test_result.loc[i, 'model'+str(j)] = sym2latex_dict[preds[j][i]]\n",
    "    print(\"Done\")    \n",
    "    return t_test_result\n",
    "\n",
    "def create_t_test_report(test_res):\n",
    "    models_names = list(test_res.columns.values[2:])\n",
    "    res = pd.DataFrame(columns=['latex','total_count']+models_names)\n",
    "    for i in range(len(symbols)):\n",
    "        res.loc[i,'latex'] = symbols.loc[i,'latex']\n",
    "        res.loc[i,'total_count'] = 0\n",
    "        for j in models_names:\n",
    "            res.loc[i,j] = 0\n",
    "    for i in range(len(test_res)):\n",
    "        res.loc[test_res.loc[i,'symbol'],'total_count'] += 1\n",
    "        for j in models_names:\n",
    "            if test_res.loc[i,'latex'] == test_res.loc[i,j]:\n",
    "                res.loc[latex2sym[test_res.loc[i,j]],j] += 1\n",
    "    for i in range(len(res)):\n",
    "        for j in models_names:\n",
    "            res.loc[i,j+'_acc'] = (res.loc[i,j]/res.loc[i,'total_count']) * 100\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stratified Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols = read_csv(processed_data_dir+'symbols.csv')\n",
    "symbols_dict = {}\n",
    "for i in range(len(symbols)):\n",
    "    symbols_dict[symbols['old_symbol'][i]] = symbols['new_id'][i]\n",
    "# symbols_list = np.array(whole_dataset['symbol_id']).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to load csvs\n",
    "train = read_csv(processed_data_dir+'train.csv').drop(['Unnamed: 0'], axis=1).reset_index(drop=True)\n",
    "test = read_csv(processed_data_dir+'test.csv').drop(['Unnamed: 0'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>latex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasy-data/v2-42303.png</td>\n",
       "      <td>70</td>\n",
       "      <td>\\mu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasy-data/v2-138273.png</td>\n",
       "      <td>102</td>\n",
       "      <td>\\infty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasy-data/v2-33731.png</td>\n",
       "      <td>63</td>\n",
       "      <td>\\gamma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasy-data/v2-47008.png</td>\n",
       "      <td>72</td>\n",
       "      <td>\\rho</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasy-data/v2-76523.png</td>\n",
       "      <td>92</td>\n",
       "      <td>\\times</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      path  symbol_id   latex\n",
       "0   hasy-data/v2-42303.png         70     \\mu\n",
       "1  hasy-data/v2-138273.png        102  \\infty\n",
       "2   hasy-data/v2-33731.png         63  \\gamma\n",
       "3   hasy-data/v2-47008.png         72    \\rho\n",
       "4   hasy-data/v2-76523.png         92  \\times"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>latex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasy-data/v2-61127.png</td>\n",
       "      <td>79</td>\n",
       "      <td>&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasy-data/v2-30654.png</td>\n",
       "      <td>55</td>\n",
       "      <td>s</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasy-data/v2-81767.png</td>\n",
       "      <td>95</td>\n",
       "      <td>\\cap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasy-data/v2-17951.png</td>\n",
       "      <td>10</td>\n",
       "      <td>K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasy-data/v2-35749.png</td>\n",
       "      <td>65</td>\n",
       "      <td>\\Delta</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     path  symbol_id   latex\n",
       "0  hasy-data/v2-61127.png         79       >\n",
       "1  hasy-data/v2-30654.png         55       s\n",
       "2  hasy-data/v2-81767.png         95    \\cap\n",
       "3  hasy-data/v2-17951.png         10       K\n",
       "4  hasy-data/v2-35749.png         65  \\Delta"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (143, 16, 89.94), 1: (55, 6, 90.16), 2: (121, 13, 90.3), 3: (52, 6, 89.66), 4: (49, 5, 90.74), 5: (50, 6, 89.29), 6: (106, 12, 89.83), 7: (58, 6, 90.62), 8: (90, 10, 90.0), 9: (94, 10, 90.38), 10: (86, 10, 89.58), 11: (100, 11, 90.09), 12: (108, 12, 90.0), 13: (95, 10, 90.48), 14: (76, 8, 90.48), 15: (65, 7, 90.28), 16: (60, 7, 89.55), 17: (75, 8, 90.36), 18: (57, 6, 90.48), 19: (50, 6, 89.29), 20: (53, 6, 89.83), 21: (50, 6, 89.29), 22: (56, 6, 90.32), 23: (49, 5, 90.74), 24: (50, 6, 89.29), 25: (59, 6, 90.77), 26: (120, 13, 90.23), 27: (106, 12, 89.83), 28: (112, 12, 90.32), 29: (108, 12, 90.0), 30: (55, 6, 90.16), 31: (70, 8, 89.74), 32: (90, 10, 90.0), 33: (68, 7, 90.67), 34: (109, 12, 90.08), 35: (81, 9, 90.0), 36: (1011, 112, 90.03), 37: (77, 9, 89.53), 38: (51, 6, 89.47), 39: (60, 7, 89.55), 40: (52, 6, 89.66), 41: (57, 6, 90.48), 42: (59, 7, 89.39), 43: (50, 6, 89.29), 44: (52, 6, 89.66), 45: (53, 6, 89.83), 46: (48, 5, 90.57), 47: (52, 6, 89.66), 48: (50, 6, 89.29), 49: (51, 6, 89.47), 50: (55, 6, 90.16), 51: (56, 6, 90.32), 52: (50, 5, 90.91), 53: (62, 7, 89.86), 54: (55, 6, 90.16), 55: (53, 6, 89.83), 56: (51, 6, 89.47), 57: (57, 6, 90.48), 58: (50, 6, 89.29), 59: (59, 7, 89.39), 60: (52, 6, 89.66), 61: (54, 6, 90.0), 62: (1719, 191, 90.0), 63: (1026, 114, 90.0), 64: (1121, 125, 89.97), 65: (896, 100, 89.96), 66: (617, 69, 89.94), 67: (586, 65, 90.02), 68: (490, 54, 90.07), 69: (896, 100, 89.96), 70: (1032, 115, 89.97), 71: (719, 80, 89.99), 72: (623, 69, 90.03), 73: (618, 69, 89.96), 74: (568, 63, 90.02), 75: (680, 75, 90.07), 76: (949, 105, 90.04), 77: (779, 87, 89.95), 78: (103, 11, 90.35), 79: (91, 10, 90.1), 80: (695, 77, 90.03), 81: (370, 41, 90.02), 82: (1418, 158, 89.97), 83: (106, 12, 89.83), 84: (81, 9, 90.0), 85: (348, 39, 89.92), 86: (1022, 114, 89.96), 87: (289, 32, 90.03), 88: (1157, 129, 89.97), 89: (960, 107, 89.97), 90: (798, 89, 89.97), 91: (831, 92, 90.03), 92: (1358, 151, 89.99), 93: (501, 56, 89.95), 94: (302, 33, 90.15), 95: (988, 110, 89.98), 96: (991, 110, 90.01), 97: (1925, 214, 90.0), 98: (1262, 140, 90.01), 99: (824, 92, 89.96), 100: (233, 26, 89.96), 101: (333, 37, 90.0), 102: (2623, 291, 90.01), 103: (297, 33, 90.0), 104: (4, 0, 100.0), 105: (6, 0, 100.0), 106: (6, 0, 100.0)}\n"
     ]
    }
   ],
   "source": [
    "# (train_count, test_count, percentage of train count to total)\n",
    "labels_count = get_label_count_train_test_dfs(train, test)\n",
    "print(labels_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encode labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T22:39:28.062035Z",
     "start_time": "2019-11-26T22:39:27.828991Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of categories is  107\n",
      "len of dataset 38444\n",
      "len of dataset 4272\n"
     ]
    }
   ],
   "source": [
    "no_categories = len(symbols)\n",
    "print('No of categories is ', no_categories)\n",
    "train_one_hot_symbols = convert_to_one_hot_encode(train['symbol_id'], no_categories)\n",
    "test_one_hot_symbols = convert_to_one_hot_encode(test['symbol_id'], no_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole_dataset['symbol_id_ohe'] = [list(one_hot_symbols[i]) for i in range(len(whole_dataset))]\n",
    "train['symbol_id_ohe'] = [list(train_one_hot_symbols[i]) for i in range(len(train))]\n",
    "test['symbol_id_ohe'] = [list(test_one_hot_symbols[i]) for i in range(len(test))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = populate_images(train)\n",
    "test = populate_images(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>latex</th>\n",
       "      <th>symbol_id_ohe</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasy-data/v2-42303.png</td>\n",
       "      <td>70</td>\n",
       "      <td>\\mu</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasy-data/v2-138273.png</td>\n",
       "      <td>102</td>\n",
       "      <td>\\infty</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasy-data/v2-33731.png</td>\n",
       "      <td>63</td>\n",
       "      <td>\\gamma</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasy-data/v2-47008.png</td>\n",
       "      <td>72</td>\n",
       "      <td>\\rho</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasy-data/v2-76523.png</td>\n",
       "      <td>92</td>\n",
       "      <td>\\times</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      path  symbol_id   latex  \\\n",
       "0   hasy-data/v2-42303.png         70     \\mu   \n",
       "1  hasy-data/v2-138273.png        102  \\infty   \n",
       "2   hasy-data/v2-33731.png         63  \\gamma   \n",
       "3   hasy-data/v2-47008.png         72    \\rho   \n",
       "4   hasy-data/v2-76523.png         92  \\times   \n",
       "\n",
       "                                       symbol_id_ohe  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                 img  \n",
       "0  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...  \n",
       "1  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...  \n",
       "2  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...  \n",
       "3  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...  \n",
       "4  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>latex</th>\n",
       "      <th>symbol_id_ohe</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasy-data/v2-61127.png</td>\n",
       "      <td>79</td>\n",
       "      <td>&gt;</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 0, 0, 0, 0, 255, 255, 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasy-data/v2-30654.png</td>\n",
       "      <td>55</td>\n",
       "      <td>s</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasy-data/v2-81767.png</td>\n",
       "      <td>95</td>\n",
       "      <td>\\cap</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasy-data/v2-17951.png</td>\n",
       "      <td>10</td>\n",
       "      <td>K</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasy-data/v2-35749.png</td>\n",
       "      <td>65</td>\n",
       "      <td>\\Delta</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[[255, 255, 255, 255, 255, 255, 255, 255, 255,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     path  symbol_id   latex  \\\n",
       "0  hasy-data/v2-61127.png         79       >   \n",
       "1  hasy-data/v2-30654.png         55       s   \n",
       "2  hasy-data/v2-81767.png         95    \\cap   \n",
       "3  hasy-data/v2-17951.png         10       K   \n",
       "4  hasy-data/v2-35749.png         65  \\Delta   \n",
       "\n",
       "                                       symbol_id_ohe  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                                 img  \n",
       "0  [[255, 255, 255, 255, 0, 0, 0, 0, 255, 255, 25...  \n",
       "1  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...  \n",
       "2  [[255, 255, 255, 255, 255, 0, 0, 0, 0, 0, 0, 0...  \n",
       "3  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...  \n",
       "4  [[255, 255, 255, 255, 255, 255, 255, 255, 255,...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T22:39:32.322770Z",
     "start_time": "2019-11-26T22:39:32.279853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Input Shape is (1, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "smooth = 1\n",
    "\n",
    "# define the channels location\n",
    "data_format = 'channels_first'\n",
    "\n",
    "# number of classification labels/classes\n",
    "classes = len(symbols)\n",
    "\n",
    "# input shape of dataset\n",
    "input_shape = (1, 32, 32)\n",
    "print(\"CNN Input Shape is\", input_shape)\n",
    "\n",
    "# optimizer\n",
    "lr = 0.001\n",
    "optimizer = SGD(lr=lr)\n",
    "\n",
    "# loss function\n",
    "loss = 'categorical_crossentropy'\n",
    "\n",
    "batch_size = 512\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with Stratified Train Test Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = process_x_y_train_test_stratified_2df(train, test, input_shape=input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create network\n",
    "model = create_network_2(input_shape, data_format, classes)\n",
    "# compile network\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 25, 32, 32)        650       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 25, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 25, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 25, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 16, 16)        11300     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 50, 16, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 50, 8, 8)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 100, 8, 8)         20100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 100, 8, 8)         32        \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100, 8, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 100, 4, 4)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 200, 4, 4)         80200     \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 200, 4, 4)         16        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 200, 4, 4)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 200, 2, 2)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3200)              2563200   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1600)              5121600   \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 107)               171307    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 107)               0         \n",
      "=================================================================\n",
      "Total params: 7,968,597\n",
      "Trainable params: 7,968,477\n",
      "Non-trainable params: 120\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Model Summary\n",
    "print(model.summary())\n",
    "#plot_model(model, to_file=dir_+'model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38444 samples, validate on 4272 samples\n",
      "Epoch 1/500\n",
      "38444/38444 [==============================] - 13s 338us/step - loss: 4.3164 - acc: 0.0552 - val_loss: 4.0669 - val_acc: 0.0681\n",
      "Epoch 2/500\n",
      "38444/38444 [==============================] - 6s 166us/step - loss: 3.9765 - acc: 0.0740 - val_loss: 3.9178 - val_acc: 0.1014\n",
      "Epoch 3/500\n",
      "38444/38444 [==============================] - 6s 167us/step - loss: 3.8512 - acc: 0.1057 - val_loss: 3.8168 - val_acc: 0.1023\n",
      "Epoch 4/500\n",
      "38444/38444 [==============================] - 6s 165us/step - loss: 3.7458 - acc: 0.1372 - val_loss: 3.7162 - val_acc: 0.1444\n",
      "Epoch 5/500\n",
      "38444/38444 [==============================] - 6s 166us/step - loss: 3.6306 - acc: 0.1824 - val_loss: 3.6042 - val_acc: 0.2203\n",
      "Epoch 6/500\n",
      "38444/38444 [==============================] - 6s 167us/step - loss: 3.5031 - acc: 0.2329 - val_loss: 3.4728 - val_acc: 0.2526\n",
      "Epoch 7/500\n",
      "38444/38444 [==============================] - 6s 167us/step - loss: 3.3650 - acc: 0.2749 - val_loss: 3.3366 - val_acc: 0.3052\n",
      "Epoch 8/500\n",
      "38444/38444 [==============================] - 6s 167us/step - loss: 3.2187 - acc: 0.3206 - val_loss: 3.1897 - val_acc: 0.3338\n",
      "Epoch 9/500\n",
      "38444/38444 [==============================] - 6s 166us/step - loss: 3.0692 - acc: 0.3620 - val_loss: 3.0427 - val_acc: 0.3769\n",
      "Epoch 10/500\n",
      "38444/38444 [==============================] - 6s 168us/step - loss: 2.9193 - acc: 0.4083 - val_loss: 2.8914 - val_acc: 0.4221\n",
      "Epoch 11/500\n",
      "38444/38444 [==============================] - 6s 168us/step - loss: 2.7696 - acc: 0.4496 - val_loss: 2.7406 - val_acc: 0.4759\n",
      "Epoch 12/500\n",
      "38444/38444 [==============================] - 6s 168us/step - loss: 2.6226 - acc: 0.4899 - val_loss: 2.5973 - val_acc: 0.5059\n",
      "Epoch 13/500\n",
      "38444/38444 [==============================] - 6s 168us/step - loss: 2.4802 - acc: 0.5215 - val_loss: 2.4578 - val_acc: 0.5368\n",
      "Epoch 14/500\n",
      "38444/38444 [==============================] - 6s 169us/step - loss: 2.3454 - acc: 0.5503 - val_loss: 2.3241 - val_acc: 0.5660\n",
      "Epoch 15/500\n",
      "38444/38444 [==============================] - 6s 169us/step - loss: 2.2172 - acc: 0.5760 - val_loss: 2.1914 - val_acc: 0.5878\n",
      "Epoch 16/500\n",
      "38444/38444 [==============================] - 6s 169us/step - loss: 2.0980 - acc: 0.5994 - val_loss: 2.0789 - val_acc: 0.6056\n",
      "Epoch 17/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 1.9879 - acc: 0.6206 - val_loss: 1.9676 - val_acc: 0.6301\n",
      "Epoch 18/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 1.8860 - acc: 0.6389 - val_loss: 1.8568 - val_acc: 0.6433\n",
      "Epoch 19/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 1.7921 - acc: 0.6571 - val_loss: 1.7664 - val_acc: 0.6571\n",
      "Epoch 20/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 1.7053 - acc: 0.6742 - val_loss: 1.6805 - val_acc: 0.6770\n",
      "Epoch 21/500\n",
      "38444/38444 [==============================] - 7s 169us/step - loss: 1.6264 - acc: 0.6919 - val_loss: 1.6059 - val_acc: 0.6992\n",
      "Epoch 22/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 1.5535 - acc: 0.7055 - val_loss: 1.5326 - val_acc: 0.7086\n",
      "Epoch 23/500\n",
      "38444/38444 [==============================] - 6s 165us/step - loss: 1.4867 - acc: 0.7178 - val_loss: 1.4647 - val_acc: 0.7240\n",
      "Epoch 24/500\n",
      "38444/38444 [==============================] - 6s 166us/step - loss: 1.4254 - acc: 0.7283 - val_loss: 1.4061 - val_acc: 0.7336\n",
      "Epoch 25/500\n",
      "38444/38444 [==============================] - 6s 166us/step - loss: 1.3691 - acc: 0.7381 - val_loss: 1.3506 - val_acc: 0.7397\n",
      "Epoch 26/500\n",
      "38444/38444 [==============================] - 6s 169us/step - loss: 1.3168 - acc: 0.7460 - val_loss: 1.2966 - val_acc: 0.7456\n",
      "Epoch 27/500\n",
      "38444/38444 [==============================] - 6s 167us/step - loss: 1.2687 - acc: 0.7525 - val_loss: 1.2465 - val_acc: 0.7528\n",
      "Epoch 28/500\n",
      "38444/38444 [==============================] - 6s 167us/step - loss: 1.2245 - acc: 0.7588 - val_loss: 1.2049 - val_acc: 0.7617\n",
      "Epoch 29/500\n",
      "38444/38444 [==============================] - 6s 168us/step - loss: 1.1833 - acc: 0.7643 - val_loss: 1.1628 - val_acc: 0.7678\n",
      "Epoch 30/500\n",
      "38444/38444 [==============================] - 6s 169us/step - loss: 1.1451 - acc: 0.7695 - val_loss: 1.1274 - val_acc: 0.7713\n",
      "Epoch 31/500\n",
      "38444/38444 [==============================] - 6s 168us/step - loss: 1.1092 - acc: 0.7739 - val_loss: 1.0940 - val_acc: 0.7718\n",
      "Epoch 32/500\n",
      "38444/38444 [==============================] - 6s 169us/step - loss: 1.0759 - acc: 0.7775 - val_loss: 1.0672 - val_acc: 0.7797\n",
      "Epoch 33/500\n",
      "38444/38444 [==============================] - 6s 168us/step - loss: 1.0437 - acc: 0.7830 - val_loss: 1.0283 - val_acc: 0.7844\n",
      "Epoch 34/500\n",
      "38444/38444 [==============================] - 7s 169us/step - loss: 1.0138 - acc: 0.7873 - val_loss: 1.0171 - val_acc: 0.7846\n",
      "Epoch 35/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.9859 - acc: 0.7916 - val_loss: 0.9769 - val_acc: 0.7910\n",
      "Epoch 36/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.9590 - acc: 0.7965 - val_loss: 0.9496 - val_acc: 0.7921\n",
      "Epoch 37/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.9340 - acc: 0.8006 - val_loss: 0.9228 - val_acc: 0.7968\n",
      "Epoch 38/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.9100 - acc: 0.8039 - val_loss: 0.9030 - val_acc: 0.8013\n",
      "Epoch 39/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.8868 - acc: 0.8088 - val_loss: 0.8821 - val_acc: 0.8038\n",
      "Epoch 40/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.8652 - acc: 0.8128 - val_loss: 0.8576 - val_acc: 0.8078\n",
      "Epoch 41/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.8441 - acc: 0.8159 - val_loss: 0.8387 - val_acc: 0.8130\n",
      "Epoch 42/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.8243 - acc: 0.8194 - val_loss: 0.8186 - val_acc: 0.8186\n",
      "Epoch 43/500\n",
      "38444/38444 [==============================] - 6s 169us/step - loss: 0.8052 - acc: 0.8227 - val_loss: 0.8023 - val_acc: 0.8165\n",
      "Epoch 44/500\n",
      "38444/38444 [==============================] - 7s 169us/step - loss: 0.7864 - acc: 0.8260 - val_loss: 0.7819 - val_acc: 0.8263\n",
      "Epoch 45/500\n",
      "38444/38444 [==============================] - 7s 169us/step - loss: 0.7693 - acc: 0.8287 - val_loss: 0.7661 - val_acc: 0.8287\n",
      "Epoch 46/500\n",
      "38444/38444 [==============================] - 7s 169us/step - loss: 0.7522 - acc: 0.8325 - val_loss: 0.7552 - val_acc: 0.8270\n",
      "Epoch 47/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.7357 - acc: 0.8355 - val_loss: 0.7327 - val_acc: 0.8343\n",
      "Epoch 48/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.7203 - acc: 0.8378 - val_loss: 0.7199 - val_acc: 0.8378\n",
      "Epoch 49/500\n",
      "38444/38444 [==============================] - 7s 169us/step - loss: 0.7052 - acc: 0.8408 - val_loss: 0.7135 - val_acc: 0.8375\n",
      "Epoch 50/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.6911 - acc: 0.8434 - val_loss: 0.7024 - val_acc: 0.8415\n",
      "Epoch 51/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.6771 - acc: 0.8455 - val_loss: 0.6790 - val_acc: 0.8441\n",
      "Epoch 52/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.6639 - acc: 0.8482 - val_loss: 0.6699 - val_acc: 0.8467\n",
      "Epoch 53/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.6509 - acc: 0.8499 - val_loss: 0.6583 - val_acc: 0.8474\n",
      "Epoch 54/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.6386 - acc: 0.8525 - val_loss: 0.6479 - val_acc: 0.8511\n",
      "Epoch 55/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.6269 - acc: 0.8554 - val_loss: 0.6543 - val_acc: 0.8467\n",
      "Epoch 56/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.6160 - acc: 0.8574 - val_loss: 0.6228 - val_acc: 0.8570\n",
      "Epoch 57/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.6044 - acc: 0.8604 - val_loss: 0.6096 - val_acc: 0.8577\n",
      "Epoch 58/500\n",
      "38444/38444 [==============================] - 7s 169us/step - loss: 0.5942 - acc: 0.8611 - val_loss: 0.6063 - val_acc: 0.8619\n",
      "Epoch 59/500\n",
      "38444/38444 [==============================] - 7s 169us/step - loss: 0.5839 - acc: 0.8635 - val_loss: 0.6009 - val_acc: 0.8610\n",
      "Epoch 60/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.5736 - acc: 0.8658 - val_loss: 0.5831 - val_acc: 0.8631\n",
      "Epoch 61/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.5644 - acc: 0.8679 - val_loss: 0.5778 - val_acc: 0.8647\n",
      "Epoch 62/500\n",
      "38444/38444 [==============================] - 7s 169us/step - loss: 0.5548 - acc: 0.8704 - val_loss: 0.5676 - val_acc: 0.8654\n",
      "Epoch 63/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.5461 - acc: 0.8718 - val_loss: 0.5569 - val_acc: 0.8701\n",
      "Epoch 64/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.5373 - acc: 0.8740 - val_loss: 0.5511 - val_acc: 0.8689\n",
      "Epoch 65/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.5290 - acc: 0.8765 - val_loss: 0.5431 - val_acc: 0.8745\n",
      "Epoch 66/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.5210 - acc: 0.8779 - val_loss: 0.5452 - val_acc: 0.8734\n",
      "Epoch 67/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.5133 - acc: 0.8796 - val_loss: 0.5267 - val_acc: 0.8741\n",
      "Epoch 68/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.5055 - acc: 0.8808 - val_loss: 0.5195 - val_acc: 0.8771\n",
      "Epoch 69/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.4982 - acc: 0.8829 - val_loss: 0.5135 - val_acc: 0.8780\n",
      "Epoch 70/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.4909 - acc: 0.8841 - val_loss: 0.5058 - val_acc: 0.8785\n",
      "Epoch 71/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.4841 - acc: 0.8862 - val_loss: 0.5024 - val_acc: 0.8806\n",
      "Epoch 72/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.4775 - acc: 0.8867 - val_loss: 0.4944 - val_acc: 0.8823\n",
      "Epoch 73/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.4708 - acc: 0.8898 - val_loss: 0.4924 - val_acc: 0.8811\n",
      "Epoch 74/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.4646 - acc: 0.8904 - val_loss: 0.4809 - val_acc: 0.8862\n",
      "Epoch 75/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.4582 - acc: 0.8919 - val_loss: 0.4857 - val_acc: 0.8811\n",
      "Epoch 76/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.4530 - acc: 0.8928 - val_loss: 0.4740 - val_acc: 0.8869\n",
      "Epoch 77/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.4468 - acc: 0.8942 - val_loss: 0.4646 - val_acc: 0.8904\n",
      "Epoch 78/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.4411 - acc: 0.8960 - val_loss: 0.4659 - val_acc: 0.8895\n",
      "Epoch 79/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.4358 - acc: 0.8968 - val_loss: 0.4573 - val_acc: 0.8881\n",
      "Epoch 80/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.4301 - acc: 0.8988 - val_loss: 0.4541 - val_acc: 0.8919\n",
      "Epoch 81/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.4254 - acc: 0.9005 - val_loss: 0.4551 - val_acc: 0.8937\n",
      "Epoch 82/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.4204 - acc: 0.9008 - val_loss: 0.4415 - val_acc: 0.8961\n",
      "Epoch 83/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.4154 - acc: 0.9023 - val_loss: 0.4406 - val_acc: 0.8949\n",
      "Epoch 84/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.4105 - acc: 0.9034 - val_loss: 0.4377 - val_acc: 0.8984\n",
      "Epoch 85/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.4059 - acc: 0.9051 - val_loss: 0.4282 - val_acc: 0.8972\n",
      "Epoch 86/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.4013 - acc: 0.9060 - val_loss: 0.4333 - val_acc: 0.8963\n",
      "Epoch 87/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.3969 - acc: 0.9073 - val_loss: 0.4223 - val_acc: 0.8998\n",
      "Epoch 88/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.3924 - acc: 0.9077 - val_loss: 0.4235 - val_acc: 0.9000\n",
      "Epoch 89/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.3884 - acc: 0.9092 - val_loss: 0.4241 - val_acc: 0.9007\n",
      "Epoch 90/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.3847 - acc: 0.9097 - val_loss: 0.4127 - val_acc: 0.8991\n",
      "Epoch 91/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.3806 - acc: 0.9107 - val_loss: 0.4082 - val_acc: 0.9026\n",
      "Epoch 92/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.3765 - acc: 0.9120 - val_loss: 0.4081 - val_acc: 0.9007\n",
      "Epoch 93/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.3727 - acc: 0.9124 - val_loss: 0.4010 - val_acc: 0.9040\n",
      "Epoch 94/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.3692 - acc: 0.9131 - val_loss: 0.4004 - val_acc: 0.9043\n",
      "Epoch 95/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.3653 - acc: 0.9142 - val_loss: 0.3977 - val_acc: 0.9031\n",
      "Epoch 96/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.3621 - acc: 0.9155 - val_loss: 0.3906 - val_acc: 0.9089\n",
      "Epoch 97/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.3586 - acc: 0.9167 - val_loss: 0.3925 - val_acc: 0.9054\n",
      "Epoch 98/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.3555 - acc: 0.9163 - val_loss: 0.3853 - val_acc: 0.9059\n",
      "Epoch 99/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.3520 - acc: 0.9176 - val_loss: 0.3853 - val_acc: 0.9043\n",
      "Epoch 100/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.3489 - acc: 0.9184 - val_loss: 0.3933 - val_acc: 0.9038\n",
      "Epoch 101/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.3461 - acc: 0.9185 - val_loss: 0.3815 - val_acc: 0.9047\n",
      "Epoch 102/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.3431 - acc: 0.9194 - val_loss: 0.3737 - val_acc: 0.9096\n",
      "Epoch 103/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.3398 - acc: 0.9205 - val_loss: 0.3735 - val_acc: 0.9092\n",
      "Epoch 104/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.3367 - acc: 0.9217 - val_loss: 0.3700 - val_acc: 0.9103\n",
      "Epoch 105/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.3338 - acc: 0.9220 - val_loss: 0.3682 - val_acc: 0.9125\n",
      "Epoch 106/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.3308 - acc: 0.9224 - val_loss: 0.3648 - val_acc: 0.9110\n",
      "Epoch 107/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.3283 - acc: 0.9232 - val_loss: 0.3607 - val_acc: 0.9125\n",
      "Epoch 108/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.3256 - acc: 0.9239 - val_loss: 0.3636 - val_acc: 0.9103\n",
      "Epoch 109/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.3232 - acc: 0.9247 - val_loss: 0.3593 - val_acc: 0.9139\n",
      "Epoch 110/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.3205 - acc: 0.9252 - val_loss: 0.3568 - val_acc: 0.9113\n",
      "Epoch 111/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.3180 - acc: 0.9264 - val_loss: 0.3597 - val_acc: 0.9110\n",
      "Epoch 112/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.3154 - acc: 0.9267 - val_loss: 0.3528 - val_acc: 0.9150\n",
      "Epoch 113/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.3132 - acc: 0.9270 - val_loss: 0.3517 - val_acc: 0.9125\n",
      "Epoch 114/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.3108 - acc: 0.9272 - val_loss: 0.3565 - val_acc: 0.9122\n",
      "Epoch 115/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.3083 - acc: 0.9274 - val_loss: 0.3614 - val_acc: 0.9085\n",
      "Epoch 116/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.3062 - acc: 0.9285 - val_loss: 0.3562 - val_acc: 0.9101\n",
      "Epoch 117/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.3041 - acc: 0.9283 - val_loss: 0.3483 - val_acc: 0.9141\n",
      "Epoch 118/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.3016 - acc: 0.9292 - val_loss: 0.3441 - val_acc: 0.9134\n",
      "Epoch 119/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2991 - acc: 0.9304 - val_loss: 0.3415 - val_acc: 0.9150\n",
      "Epoch 120/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.2974 - acc: 0.9302 - val_loss: 0.3443 - val_acc: 0.9136\n",
      "Epoch 121/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2951 - acc: 0.9309 - val_loss: 0.3521 - val_acc: 0.9132\n",
      "Epoch 122/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2932 - acc: 0.9311 - val_loss: 0.3334 - val_acc: 0.9188\n",
      "Epoch 123/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2911 - acc: 0.9314 - val_loss: 0.3355 - val_acc: 0.9169\n",
      "Epoch 124/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.2890 - acc: 0.9321 - val_loss: 0.3344 - val_acc: 0.9188\n",
      "Epoch 125/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2871 - acc: 0.9325 - val_loss: 0.3351 - val_acc: 0.9167\n",
      "Epoch 126/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2851 - acc: 0.9329 - val_loss: 0.3326 - val_acc: 0.9176\n",
      "Epoch 127/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.2833 - acc: 0.9328 - val_loss: 0.3294 - val_acc: 0.9164\n",
      "Epoch 128/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2811 - acc: 0.9341 - val_loss: 0.3255 - val_acc: 0.9188\n",
      "Epoch 129/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2794 - acc: 0.9338 - val_loss: 0.3275 - val_acc: 0.9202\n",
      "Epoch 130/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2777 - acc: 0.9345 - val_loss: 0.3236 - val_acc: 0.9199\n",
      "Epoch 131/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2758 - acc: 0.9350 - val_loss: 0.3235 - val_acc: 0.9197\n",
      "Epoch 132/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.2742 - acc: 0.9350 - val_loss: 0.3228 - val_acc: 0.9192\n",
      "Epoch 133/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2723 - acc: 0.9358 - val_loss: 0.3245 - val_acc: 0.9169\n",
      "Epoch 134/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.2706 - acc: 0.9360 - val_loss: 0.3181 - val_acc: 0.9213\n",
      "Epoch 135/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2689 - acc: 0.9359 - val_loss: 0.3133 - val_acc: 0.9228\n",
      "Epoch 136/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.2672 - acc: 0.9369 - val_loss: 0.3162 - val_acc: 0.9185\n",
      "Epoch 137/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2657 - acc: 0.9370 - val_loss: 0.3151 - val_acc: 0.9202\n",
      "Epoch 138/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.2640 - acc: 0.9376 - val_loss: 0.3304 - val_acc: 0.9160\n",
      "Epoch 139/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2625 - acc: 0.9382 - val_loss: 0.3174 - val_acc: 0.9183\n",
      "Epoch 140/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2609 - acc: 0.9383 - val_loss: 0.3084 - val_acc: 0.9239\n",
      "Epoch 141/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2591 - acc: 0.9384 - val_loss: 0.3073 - val_acc: 0.9225\n",
      "Epoch 142/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2580 - acc: 0.9388 - val_loss: 0.3098 - val_acc: 0.9225\n",
      "Epoch 143/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2561 - acc: 0.9393 - val_loss: 0.3093 - val_acc: 0.9225\n",
      "Epoch 144/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2547 - acc: 0.9394 - val_loss: 0.3101 - val_acc: 0.9211\n",
      "Epoch 145/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2533 - acc: 0.9400 - val_loss: 0.3048 - val_acc: 0.9232\n",
      "Epoch 146/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2518 - acc: 0.9406 - val_loss: 0.3025 - val_acc: 0.9260\n",
      "Epoch 147/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2502 - acc: 0.9405 - val_loss: 0.3115 - val_acc: 0.9211\n",
      "Epoch 148/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2490 - acc: 0.9406 - val_loss: 0.3007 - val_acc: 0.9246\n",
      "Epoch 149/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2477 - acc: 0.9413 - val_loss: 0.3028 - val_acc: 0.9223\n",
      "Epoch 150/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2464 - acc: 0.9421 - val_loss: 0.3001 - val_acc: 0.9239\n",
      "Epoch 151/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2448 - acc: 0.9417 - val_loss: 0.2961 - val_acc: 0.9267\n",
      "Epoch 152/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2433 - acc: 0.9423 - val_loss: 0.2973 - val_acc: 0.9246\n",
      "Epoch 153/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2423 - acc: 0.9425 - val_loss: 0.2996 - val_acc: 0.9237\n",
      "Epoch 154/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2409 - acc: 0.9432 - val_loss: 0.2971 - val_acc: 0.9237\n",
      "Epoch 155/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2396 - acc: 0.9435 - val_loss: 0.2957 - val_acc: 0.9249\n",
      "Epoch 156/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2382 - acc: 0.9432 - val_loss: 0.2971 - val_acc: 0.9230\n",
      "Epoch 157/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2369 - acc: 0.9439 - val_loss: 0.2927 - val_acc: 0.9246\n",
      "Epoch 158/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.2356 - acc: 0.9439 - val_loss: 0.2925 - val_acc: 0.9265\n",
      "Epoch 159/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.2346 - acc: 0.9443 - val_loss: 0.2905 - val_acc: 0.9246\n",
      "Epoch 160/500\n",
      "38444/38444 [==============================] - 7s 179us/step - loss: 0.2330 - acc: 0.9450 - val_loss: 0.2924 - val_acc: 0.9249\n",
      "Epoch 161/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.2319 - acc: 0.9451 - val_loss: 0.2880 - val_acc: 0.9274\n",
      "Epoch 162/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2309 - acc: 0.9450 - val_loss: 0.2909 - val_acc: 0.9258\n",
      "Epoch 163/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2299 - acc: 0.9460 - val_loss: 0.2899 - val_acc: 0.9251\n",
      "Epoch 164/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2283 - acc: 0.9455 - val_loss: 0.2878 - val_acc: 0.9272\n",
      "Epoch 165/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2274 - acc: 0.9462 - val_loss: 0.2937 - val_acc: 0.9256\n",
      "Epoch 166/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2261 - acc: 0.9471 - val_loss: 0.2852 - val_acc: 0.9267\n",
      "Epoch 167/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.2250 - acc: 0.9469 - val_loss: 0.2884 - val_acc: 0.9270\n",
      "Epoch 168/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2239 - acc: 0.9474 - val_loss: 0.2849 - val_acc: 0.9274\n",
      "Epoch 169/500\n",
      "38444/38444 [==============================] - 7s 179us/step - loss: 0.2227 - acc: 0.9475 - val_loss: 0.2881 - val_acc: 0.9272\n",
      "Epoch 170/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.2216 - acc: 0.9475 - val_loss: 0.2849 - val_acc: 0.9270\n",
      "Epoch 171/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.2204 - acc: 0.9478 - val_loss: 0.2815 - val_acc: 0.9284\n",
      "Epoch 172/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2193 - acc: 0.9476 - val_loss: 0.2803 - val_acc: 0.9272\n",
      "Epoch 173/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2181 - acc: 0.9484 - val_loss: 0.2865 - val_acc: 0.9260\n",
      "Epoch 174/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.2174 - acc: 0.9487 - val_loss: 0.2801 - val_acc: 0.9291\n",
      "Epoch 175/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2161 - acc: 0.9488 - val_loss: 0.2791 - val_acc: 0.9265\n",
      "Epoch 176/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.2150 - acc: 0.9494 - val_loss: 0.2848 - val_acc: 0.9246\n",
      "Epoch 177/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2142 - acc: 0.9490 - val_loss: 0.2828 - val_acc: 0.9253\n",
      "Epoch 178/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2133 - acc: 0.9493 - val_loss: 0.2760 - val_acc: 0.9291\n",
      "Epoch 179/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2119 - acc: 0.9498 - val_loss: 0.2765 - val_acc: 0.9293\n",
      "Epoch 180/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.2107 - acc: 0.9501 - val_loss: 0.2748 - val_acc: 0.9286\n",
      "Epoch 181/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.2098 - acc: 0.9503 - val_loss: 0.2786 - val_acc: 0.9307\n",
      "Epoch 182/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.2092 - acc: 0.9506 - val_loss: 0.2786 - val_acc: 0.9291\n",
      "Epoch 183/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.2081 - acc: 0.9509 - val_loss: 0.2753 - val_acc: 0.9288\n",
      "Epoch 184/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.2070 - acc: 0.9512 - val_loss: 0.2737 - val_acc: 0.9279\n",
      "Epoch 185/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.2060 - acc: 0.9514 - val_loss: 0.2708 - val_acc: 0.9298\n",
      "Epoch 186/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.2052 - acc: 0.9510 - val_loss: 0.2747 - val_acc: 0.9295\n",
      "Epoch 187/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.2042 - acc: 0.9520 - val_loss: 0.2730 - val_acc: 0.9274\n",
      "Epoch 188/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.2032 - acc: 0.9517 - val_loss: 0.2708 - val_acc: 0.9305\n",
      "Epoch 189/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.2018 - acc: 0.9528 - val_loss: 0.2750 - val_acc: 0.9295\n",
      "Epoch 190/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.2015 - acc: 0.9527 - val_loss: 0.2767 - val_acc: 0.9277\n",
      "Epoch 191/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.2006 - acc: 0.9525 - val_loss: 0.2702 - val_acc: 0.9288\n",
      "Epoch 192/500\n",
      "38444/38444 [==============================] - 7s 170us/step - loss: 0.1994 - acc: 0.9532 - val_loss: 0.2754 - val_acc: 0.9288\n",
      "Epoch 193/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1990 - acc: 0.9528 - val_loss: 0.2712 - val_acc: 0.9288\n",
      "Epoch 194/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1975 - acc: 0.9536 - val_loss: 0.2745 - val_acc: 0.9286\n",
      "Epoch 195/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1969 - acc: 0.9535 - val_loss: 0.2725 - val_acc: 0.9300\n",
      "Epoch 196/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1962 - acc: 0.9540 - val_loss: 0.2669 - val_acc: 0.9314\n",
      "Epoch 197/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1948 - acc: 0.9542 - val_loss: 0.2672 - val_acc: 0.9312\n",
      "Epoch 198/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1942 - acc: 0.9542 - val_loss: 0.2658 - val_acc: 0.9312\n",
      "Epoch 199/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1932 - acc: 0.9547 - val_loss: 0.2664 - val_acc: 0.9312\n",
      "Epoch 200/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1923 - acc: 0.9548 - val_loss: 0.2635 - val_acc: 0.9307\n",
      "Epoch 201/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1915 - acc: 0.9553 - val_loss: 0.2680 - val_acc: 0.9314\n",
      "Epoch 202/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1908 - acc: 0.9551 - val_loss: 0.2662 - val_acc: 0.9321\n",
      "Epoch 203/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1901 - acc: 0.9548 - val_loss: 0.2626 - val_acc: 0.9316\n",
      "Epoch 204/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1889 - acc: 0.9560 - val_loss: 0.2624 - val_acc: 0.9324\n",
      "Epoch 205/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1881 - acc: 0.9559 - val_loss: 0.2656 - val_acc: 0.9302\n",
      "Epoch 206/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1873 - acc: 0.9555 - val_loss: 0.2669 - val_acc: 0.9316\n",
      "Epoch 207/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.1865 - acc: 0.9563 - val_loss: 0.2633 - val_acc: 0.9321\n",
      "Epoch 208/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1858 - acc: 0.9570 - val_loss: 0.2610 - val_acc: 0.9312\n",
      "Epoch 209/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1849 - acc: 0.9570 - val_loss: 0.2637 - val_acc: 0.9305\n",
      "Epoch 210/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1842 - acc: 0.9569 - val_loss: 0.2587 - val_acc: 0.9326\n",
      "Epoch 211/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1833 - acc: 0.9568 - val_loss: 0.2632 - val_acc: 0.9316\n",
      "Epoch 212/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1825 - acc: 0.9573 - val_loss: 0.2641 - val_acc: 0.9335\n",
      "Epoch 213/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1818 - acc: 0.9574 - val_loss: 0.2670 - val_acc: 0.9286\n",
      "Epoch 214/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1810 - acc: 0.9584 - val_loss: 0.2615 - val_acc: 0.9326\n",
      "Epoch 215/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1802 - acc: 0.9581 - val_loss: 0.2611 - val_acc: 0.9319\n",
      "Epoch 216/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1794 - acc: 0.9581 - val_loss: 0.2588 - val_acc: 0.9338\n",
      "Epoch 217/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1785 - acc: 0.9581 - val_loss: 0.2687 - val_acc: 0.9295\n",
      "Epoch 218/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1783 - acc: 0.9586 - val_loss: 0.2579 - val_acc: 0.9328\n",
      "Epoch 219/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1771 - acc: 0.9589 - val_loss: 0.2616 - val_acc: 0.9328\n",
      "Epoch 220/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1765 - acc: 0.9586 - val_loss: 0.2554 - val_acc: 0.9331\n",
      "Epoch 221/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1756 - acc: 0.9588 - val_loss: 0.2577 - val_acc: 0.9335\n",
      "Epoch 222/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1751 - acc: 0.9592 - val_loss: 0.2584 - val_acc: 0.9333\n",
      "Epoch 223/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1742 - acc: 0.9596 - val_loss: 0.2565 - val_acc: 0.9331\n",
      "Epoch 224/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1735 - acc: 0.9599 - val_loss: 0.2557 - val_acc: 0.9335\n",
      "Epoch 225/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1729 - acc: 0.9601 - val_loss: 0.2635 - val_acc: 0.9307\n",
      "Epoch 226/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1724 - acc: 0.9599 - val_loss: 0.2562 - val_acc: 0.9326\n",
      "Epoch 227/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1713 - acc: 0.9603 - val_loss: 0.2541 - val_acc: 0.9338\n",
      "Epoch 228/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1706 - acc: 0.9602 - val_loss: 0.2575 - val_acc: 0.9342\n",
      "Epoch 229/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1702 - acc: 0.9608 - val_loss: 0.2575 - val_acc: 0.9326\n",
      "Epoch 230/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1694 - acc: 0.9607 - val_loss: 0.2526 - val_acc: 0.9326\n",
      "Epoch 231/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1687 - acc: 0.9614 - val_loss: 0.2605 - val_acc: 0.9333\n",
      "Epoch 232/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1682 - acc: 0.9607 - val_loss: 0.2524 - val_acc: 0.9335\n",
      "Epoch 233/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1673 - acc: 0.9609 - val_loss: 0.2727 - val_acc: 0.9277\n",
      "Epoch 234/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1671 - acc: 0.9611 - val_loss: 0.2544 - val_acc: 0.9349\n",
      "Epoch 235/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1659 - acc: 0.9619 - val_loss: 0.2571 - val_acc: 0.9331\n",
      "Epoch 236/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1653 - acc: 0.9617 - val_loss: 0.2532 - val_acc: 0.9338\n",
      "Epoch 237/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1642 - acc: 0.9622 - val_loss: 0.2571 - val_acc: 0.9340\n",
      "Epoch 238/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1644 - acc: 0.9618 - val_loss: 0.2506 - val_acc: 0.9335\n",
      "Epoch 239/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1631 - acc: 0.9623 - val_loss: 0.2569 - val_acc: 0.9338\n",
      "Epoch 240/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1625 - acc: 0.9626 - val_loss: 0.2491 - val_acc: 0.9352\n",
      "Epoch 241/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1617 - acc: 0.9631 - val_loss: 0.2504 - val_acc: 0.9345\n",
      "Epoch 242/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1614 - acc: 0.9627 - val_loss: 0.2515 - val_acc: 0.9352\n",
      "Epoch 243/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1607 - acc: 0.9629 - val_loss: 0.2503 - val_acc: 0.9354\n",
      "Epoch 244/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1600 - acc: 0.9634 - val_loss: 0.2543 - val_acc: 0.9328\n",
      "Epoch 245/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1593 - acc: 0.9632 - val_loss: 0.2632 - val_acc: 0.9307\n",
      "Epoch 246/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1589 - acc: 0.9629 - val_loss: 0.2543 - val_acc: 0.9328\n",
      "Epoch 247/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.1578 - acc: 0.9636 - val_loss: 0.2602 - val_acc: 0.9314\n",
      "Epoch 248/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1575 - acc: 0.9635 - val_loss: 0.2472 - val_acc: 0.9356\n",
      "Epoch 249/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1568 - acc: 0.9640 - val_loss: 0.2470 - val_acc: 0.9356\n",
      "Epoch 250/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1559 - acc: 0.9644 - val_loss: 0.2470 - val_acc: 0.9356\n",
      "Epoch 251/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1555 - acc: 0.9643 - val_loss: 0.2520 - val_acc: 0.9352\n",
      "Epoch 252/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1549 - acc: 0.9644 - val_loss: 0.2579 - val_acc: 0.9319\n",
      "Epoch 253/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1547 - acc: 0.9646 - val_loss: 0.2473 - val_acc: 0.9366\n",
      "Epoch 254/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1539 - acc: 0.9646 - val_loss: 0.2541 - val_acc: 0.9345\n",
      "Epoch 255/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1531 - acc: 0.9650 - val_loss: 0.2495 - val_acc: 0.9345\n",
      "Epoch 256/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1527 - acc: 0.9650 - val_loss: 0.2437 - val_acc: 0.9349\n",
      "Epoch 257/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1518 - acc: 0.9656 - val_loss: 0.2462 - val_acc: 0.9361\n",
      "Epoch 258/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1515 - acc: 0.9651 - val_loss: 0.2472 - val_acc: 0.9361\n",
      "Epoch 259/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1507 - acc: 0.9660 - val_loss: 0.2514 - val_acc: 0.9349\n",
      "Epoch 260/500\n",
      "38444/38444 [==============================] - 7s 171us/step - loss: 0.1502 - acc: 0.9656 - val_loss: 0.2436 - val_acc: 0.9361\n",
      "Epoch 261/500\n",
      "38444/38444 [==============================] - 7s 172us/step - loss: 0.1496 - acc: 0.9661 - val_loss: 0.2444 - val_acc: 0.9363\n",
      "Epoch 262/500\n",
      "38444/38444 [==============================] - 7s 188us/step - loss: 0.1490 - acc: 0.9663 - val_loss: 0.2445 - val_acc: 0.9361\n",
      "Epoch 263/500\n",
      "38444/38444 [==============================] - 8s 202us/step - loss: 0.1482 - acc: 0.9665 - val_loss: 0.2494 - val_acc: 0.9361\n",
      "Epoch 264/500\n",
      "38444/38444 [==============================] - 8s 202us/step - loss: 0.1477 - acc: 0.9670 - val_loss: 0.2511 - val_acc: 0.9349\n",
      "Epoch 265/500\n",
      "38444/38444 [==============================] - 8s 204us/step - loss: 0.1471 - acc: 0.9662 - val_loss: 0.2435 - val_acc: 0.9359\n",
      "Epoch 266/500\n",
      "38444/38444 [==============================] - 8s 205us/step - loss: 0.1467 - acc: 0.9665 - val_loss: 0.2464 - val_acc: 0.9363\n",
      "Epoch 267/500\n",
      "38444/38444 [==============================] - 8s 205us/step - loss: 0.1461 - acc: 0.9674 - val_loss: 0.2423 - val_acc: 0.9368\n",
      "Epoch 268/500\n",
      "38444/38444 [==============================] - 8s 205us/step - loss: 0.1456 - acc: 0.9668 - val_loss: 0.2463 - val_acc: 0.9382\n",
      "Epoch 269/500\n",
      "38444/38444 [==============================] - 8s 205us/step - loss: 0.1452 - acc: 0.9675 - val_loss: 0.2431 - val_acc: 0.9363\n",
      "Epoch 270/500\n",
      "38444/38444 [==============================] - 8s 205us/step - loss: 0.1443 - acc: 0.9673 - val_loss: 0.2421 - val_acc: 0.9363\n",
      "Epoch 271/500\n",
      "38444/38444 [==============================] - 8s 205us/step - loss: 0.1438 - acc: 0.9679 - val_loss: 0.2408 - val_acc: 0.9366\n",
      "Epoch 272/500\n",
      "38444/38444 [==============================] - 8s 205us/step - loss: 0.1431 - acc: 0.9675 - val_loss: 0.2482 - val_acc: 0.9370\n",
      "Epoch 273/500\n",
      "38444/38444 [==============================] - 8s 206us/step - loss: 0.1431 - acc: 0.9678 - val_loss: 0.2421 - val_acc: 0.9373\n",
      "Epoch 274/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1424 - acc: 0.9676 - val_loss: 0.2582 - val_acc: 0.9338\n",
      "Epoch 275/500\n",
      "38444/38444 [==============================] - 8s 206us/step - loss: 0.1421 - acc: 0.9684 - val_loss: 0.2408 - val_acc: 0.9354\n",
      "Epoch 276/500\n",
      "38444/38444 [==============================] - 8s 205us/step - loss: 0.1411 - acc: 0.9685 - val_loss: 0.2428 - val_acc: 0.9361\n",
      "Epoch 277/500\n",
      "38444/38444 [==============================] - 8s 196us/step - loss: 0.1404 - acc: 0.9689 - val_loss: 0.2426 - val_acc: 0.9361\n",
      "Epoch 278/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.1399 - acc: 0.9691 - val_loss: 0.2471 - val_acc: 0.9352\n",
      "Epoch 279/500\n",
      "38444/38444 [==============================] - 9s 231us/step - loss: 0.1393 - acc: 0.9691 - val_loss: 0.2684 - val_acc: 0.9314\n",
      "Epoch 280/500\n",
      "38444/38444 [==============================] - 8s 204us/step - loss: 0.1396 - acc: 0.9689 - val_loss: 0.2404 - val_acc: 0.9375\n",
      "Epoch 281/500\n",
      "38444/38444 [==============================] - 8s 204us/step - loss: 0.1383 - acc: 0.9697 - val_loss: 0.2468 - val_acc: 0.9349\n",
      "Epoch 282/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1382 - acc: 0.9690 - val_loss: 0.2386 - val_acc: 0.9373\n",
      "Epoch 283/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1375 - acc: 0.9694 - val_loss: 0.2455 - val_acc: 0.9363\n",
      "Epoch 284/500\n",
      "38444/38444 [==============================] - 8s 211us/step - loss: 0.1369 - acc: 0.9701 - val_loss: 0.2492 - val_acc: 0.9359\n",
      "Epoch 285/500\n",
      "38444/38444 [==============================] - 8s 220us/step - loss: 0.1364 - acc: 0.9696 - val_loss: 0.2393 - val_acc: 0.9363\n",
      "Epoch 286/500\n",
      "38444/38444 [==============================] - 8s 221us/step - loss: 0.1356 - acc: 0.9702 - val_loss: 0.2388 - val_acc: 0.9380\n",
      "Epoch 287/500\n",
      "38444/38444 [==============================] - 8s 216us/step - loss: 0.1352 - acc: 0.9702 - val_loss: 0.2507 - val_acc: 0.9361\n",
      "Epoch 288/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1349 - acc: 0.9704 - val_loss: 0.2475 - val_acc: 0.9359\n",
      "Epoch 289/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1346 - acc: 0.9706 - val_loss: 0.2383 - val_acc: 0.9373\n",
      "Epoch 290/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1339 - acc: 0.9707 - val_loss: 0.2485 - val_acc: 0.9361\n",
      "Epoch 291/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1332 - acc: 0.9712 - val_loss: 0.2384 - val_acc: 0.9370\n",
      "Epoch 292/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1327 - acc: 0.9710 - val_loss: 0.2423 - val_acc: 0.9354\n",
      "Epoch 293/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1322 - acc: 0.9709 - val_loss: 0.2364 - val_acc: 0.9384\n",
      "Epoch 294/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1318 - acc: 0.9710 - val_loss: 0.2433 - val_acc: 0.9354\n",
      "Epoch 295/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1313 - acc: 0.9714 - val_loss: 0.2395 - val_acc: 0.9375\n",
      "Epoch 296/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1309 - acc: 0.9715 - val_loss: 0.2372 - val_acc: 0.9375\n",
      "Epoch 297/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1305 - acc: 0.9713 - val_loss: 0.2498 - val_acc: 0.9356\n",
      "Epoch 298/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1299 - acc: 0.9718 - val_loss: 0.2370 - val_acc: 0.9377\n",
      "Epoch 299/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1295 - acc: 0.9719 - val_loss: 0.2409 - val_acc: 0.9373\n",
      "Epoch 300/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1289 - acc: 0.9719 - val_loss: 0.2448 - val_acc: 0.9356\n",
      "Epoch 301/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1285 - acc: 0.9721 - val_loss: 0.2369 - val_acc: 0.9380\n",
      "Epoch 302/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1278 - acc: 0.9723 - val_loss: 0.2362 - val_acc: 0.9384\n",
      "Epoch 303/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1273 - acc: 0.9722 - val_loss: 0.2383 - val_acc: 0.9391\n",
      "Epoch 304/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1268 - acc: 0.9728 - val_loss: 0.2384 - val_acc: 0.9387\n",
      "Epoch 305/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1264 - acc: 0.9728 - val_loss: 0.2433 - val_acc: 0.9375\n",
      "Epoch 306/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1259 - acc: 0.9726 - val_loss: 0.2360 - val_acc: 0.9394\n",
      "Epoch 307/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1255 - acc: 0.9729 - val_loss: 0.2416 - val_acc: 0.9370\n",
      "Epoch 308/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1254 - acc: 0.9727 - val_loss: 0.2341 - val_acc: 0.9408\n",
      "Epoch 309/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1245 - acc: 0.9733 - val_loss: 0.2381 - val_acc: 0.9370\n",
      "Epoch 310/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1242 - acc: 0.9739 - val_loss: 0.2349 - val_acc: 0.9410\n",
      "Epoch 311/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1235 - acc: 0.9735 - val_loss: 0.2368 - val_acc: 0.9394\n",
      "Epoch 312/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1234 - acc: 0.9736 - val_loss: 0.2396 - val_acc: 0.9373\n",
      "Epoch 313/500\n",
      "38444/38444 [==============================] - 8s 207us/step - loss: 0.1228 - acc: 0.9733 - val_loss: 0.2377 - val_acc: 0.9366\n",
      "Epoch 314/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1225 - acc: 0.9736 - val_loss: 0.2357 - val_acc: 0.9391\n",
      "Epoch 315/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1218 - acc: 0.9744 - val_loss: 0.2371 - val_acc: 0.9380\n",
      "Epoch 316/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1214 - acc: 0.9738 - val_loss: 0.2335 - val_acc: 0.9391\n",
      "Epoch 317/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1211 - acc: 0.9742 - val_loss: 0.2363 - val_acc: 0.9389\n",
      "Epoch 318/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1204 - acc: 0.9739 - val_loss: 0.2355 - val_acc: 0.9384\n",
      "Epoch 319/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1202 - acc: 0.9744 - val_loss: 0.2362 - val_acc: 0.9384\n",
      "Epoch 320/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1197 - acc: 0.9745 - val_loss: 0.2391 - val_acc: 0.9389\n",
      "Epoch 321/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1192 - acc: 0.9745 - val_loss: 0.2353 - val_acc: 0.9373\n",
      "Epoch 322/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1188 - acc: 0.9747 - val_loss: 0.2330 - val_acc: 0.9415\n",
      "Epoch 323/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.1183 - acc: 0.9748 - val_loss: 0.2468 - val_acc: 0.9354\n",
      "Epoch 324/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1181 - acc: 0.9745 - val_loss: 0.2326 - val_acc: 0.9387\n",
      "Epoch 325/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1174 - acc: 0.9751 - val_loss: 0.2347 - val_acc: 0.9398\n",
      "Epoch 326/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1173 - acc: 0.9747 - val_loss: 0.2366 - val_acc: 0.9394\n",
      "Epoch 327/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1168 - acc: 0.9747 - val_loss: 0.2316 - val_acc: 0.9410\n",
      "Epoch 328/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1163 - acc: 0.9753 - val_loss: 0.2439 - val_acc: 0.9349\n",
      "Epoch 329/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1164 - acc: 0.9753 - val_loss: 0.2339 - val_acc: 0.9380\n",
      "Epoch 330/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1151 - acc: 0.9755 - val_loss: 0.2393 - val_acc: 0.9368\n",
      "Epoch 331/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1151 - acc: 0.9757 - val_loss: 0.2365 - val_acc: 0.9377\n",
      "Epoch 332/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1146 - acc: 0.9760 - val_loss: 0.2422 - val_acc: 0.9373\n",
      "Epoch 333/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1142 - acc: 0.9757 - val_loss: 0.2322 - val_acc: 0.9419\n",
      "Epoch 334/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1137 - acc: 0.9759 - val_loss: 0.2356 - val_acc: 0.9387\n",
      "Epoch 335/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1136 - acc: 0.9759 - val_loss: 0.2357 - val_acc: 0.9405\n",
      "Epoch 336/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1129 - acc: 0.9761 - val_loss: 0.2335 - val_acc: 0.9389\n",
      "Epoch 337/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1126 - acc: 0.9760 - val_loss: 0.2376 - val_acc: 0.9382\n",
      "Epoch 338/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1120 - acc: 0.9763 - val_loss: 0.2340 - val_acc: 0.9384\n",
      "Epoch 339/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1114 - acc: 0.9765 - val_loss: 0.2326 - val_acc: 0.9410\n",
      "Epoch 340/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1112 - acc: 0.9768 - val_loss: 0.2325 - val_acc: 0.9394\n",
      "Epoch 341/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1109 - acc: 0.9763 - val_loss: 0.2358 - val_acc: 0.9387\n",
      "Epoch 342/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1104 - acc: 0.9768 - val_loss: 0.2323 - val_acc: 0.9389\n",
      "Epoch 343/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1097 - acc: 0.9767 - val_loss: 0.2411 - val_acc: 0.9363\n",
      "Epoch 344/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1096 - acc: 0.9774 - val_loss: 0.2307 - val_acc: 0.9405\n",
      "Epoch 345/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1090 - acc: 0.9773 - val_loss: 0.2369 - val_acc: 0.9380\n",
      "Epoch 346/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1089 - acc: 0.9768 - val_loss: 0.2308 - val_acc: 0.9419\n",
      "Epoch 347/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1085 - acc: 0.9776 - val_loss: 0.2329 - val_acc: 0.9401\n",
      "Epoch 348/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1080 - acc: 0.9771 - val_loss: 0.2305 - val_acc: 0.9415\n",
      "Epoch 349/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1077 - acc: 0.9774 - val_loss: 0.2380 - val_acc: 0.9391\n",
      "Epoch 350/500\n",
      "38444/38444 [==============================] - 8s 211us/step - loss: 0.1074 - acc: 0.9778 - val_loss: 0.2339 - val_acc: 0.9384\n",
      "Epoch 351/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1068 - acc: 0.9777 - val_loss: 0.2331 - val_acc: 0.9410\n",
      "Epoch 352/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1066 - acc: 0.9780 - val_loss: 0.2349 - val_acc: 0.9387\n",
      "Epoch 353/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1061 - acc: 0.9782 - val_loss: 0.2348 - val_acc: 0.9384\n",
      "Epoch 354/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1057 - acc: 0.9781 - val_loss: 0.2310 - val_acc: 0.9398\n",
      "Epoch 355/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1054 - acc: 0.9780 - val_loss: 0.2299 - val_acc: 0.9410\n",
      "Epoch 356/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1048 - acc: 0.9783 - val_loss: 0.2356 - val_acc: 0.9377\n",
      "Epoch 357/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1045 - acc: 0.9786 - val_loss: 0.2300 - val_acc: 0.9401\n",
      "Epoch 358/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.1041 - acc: 0.9786 - val_loss: 0.2300 - val_acc: 0.9396\n",
      "Epoch 359/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1037 - acc: 0.9783 - val_loss: 0.2337 - val_acc: 0.9403\n",
      "Epoch 360/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1035 - acc: 0.9787 - val_loss: 0.2312 - val_acc: 0.9410\n",
      "Epoch 361/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1030 - acc: 0.9787 - val_loss: 0.2303 - val_acc: 0.9394\n",
      "Epoch 362/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1027 - acc: 0.9790 - val_loss: 0.2334 - val_acc: 0.9394\n",
      "Epoch 363/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1024 - acc: 0.9790 - val_loss: 0.2292 - val_acc: 0.9403\n",
      "Epoch 364/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1020 - acc: 0.9790 - val_loss: 0.2314 - val_acc: 0.9398\n",
      "Epoch 365/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1015 - acc: 0.9793 - val_loss: 0.2316 - val_acc: 0.9389\n",
      "Epoch 366/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1010 - acc: 0.9793 - val_loss: 0.2300 - val_acc: 0.9394\n",
      "Epoch 367/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1008 - acc: 0.9790 - val_loss: 0.2328 - val_acc: 0.9366\n",
      "Epoch 368/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1004 - acc: 0.9798 - val_loss: 0.2304 - val_acc: 0.9408\n",
      "Epoch 369/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.1001 - acc: 0.9795 - val_loss: 0.2303 - val_acc: 0.9412\n",
      "Epoch 370/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.0998 - acc: 0.9798 - val_loss: 0.2314 - val_acc: 0.9394\n",
      "Epoch 371/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.0995 - acc: 0.9797 - val_loss: 0.2290 - val_acc: 0.9412\n",
      "Epoch 372/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.0990 - acc: 0.9801 - val_loss: 0.2283 - val_acc: 0.9422\n",
      "Epoch 373/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.0986 - acc: 0.9795 - val_loss: 0.2307 - val_acc: 0.9394\n",
      "Epoch 374/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.0983 - acc: 0.9803 - val_loss: 0.2316 - val_acc: 0.9398\n",
      "Epoch 375/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.0979 - acc: 0.9798 - val_loss: 0.2285 - val_acc: 0.9417\n",
      "Epoch 376/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.0974 - acc: 0.9805 - val_loss: 0.2313 - val_acc: 0.9387\n",
      "Epoch 377/500\n",
      "38444/38444 [==============================] - 8s 214us/step - loss: 0.0972 - acc: 0.9803 - val_loss: 0.2302 - val_acc: 0.9405\n",
      "Epoch 378/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0970 - acc: 0.9802 - val_loss: 0.2275 - val_acc: 0.9417\n",
      "Epoch 379/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.0964 - acc: 0.9805 - val_loss: 0.2283 - val_acc: 0.9422\n",
      "Epoch 380/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.0962 - acc: 0.9806 - val_loss: 0.2293 - val_acc: 0.9417\n",
      "Epoch 381/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.0960 - acc: 0.9804 - val_loss: 0.2285 - val_acc: 0.9415\n",
      "Epoch 382/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.0957 - acc: 0.9810 - val_loss: 0.2312 - val_acc: 0.9412\n",
      "Epoch 383/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0951 - acc: 0.9811 - val_loss: 0.2305 - val_acc: 0.9415\n",
      "Epoch 384/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.0949 - acc: 0.9811 - val_loss: 0.2278 - val_acc: 0.9424\n",
      "Epoch 385/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0943 - acc: 0.9814 - val_loss: 0.2328 - val_acc: 0.9384\n",
      "Epoch 386/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0942 - acc: 0.9811 - val_loss: 0.2299 - val_acc: 0.9417\n",
      "Epoch 387/500\n",
      "38444/38444 [==============================] - 8s 210us/step - loss: 0.0938 - acc: 0.9811 - val_loss: 0.2348 - val_acc: 0.9387\n",
      "Epoch 388/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0939 - acc: 0.9812 - val_loss: 0.2369 - val_acc: 0.9342\n",
      "Epoch 389/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0934 - acc: 0.9811 - val_loss: 0.2277 - val_acc: 0.9415\n",
      "Epoch 390/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0928 - acc: 0.9816 - val_loss: 0.2306 - val_acc: 0.9410\n",
      "Epoch 391/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0926 - acc: 0.9815 - val_loss: 0.2288 - val_acc: 0.9408\n",
      "Epoch 392/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0919 - acc: 0.9817 - val_loss: 0.2276 - val_acc: 0.9410\n",
      "Epoch 393/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0915 - acc: 0.9816 - val_loss: 0.2317 - val_acc: 0.9396\n",
      "Epoch 394/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0914 - acc: 0.9817 - val_loss: 0.2279 - val_acc: 0.9410\n",
      "Epoch 395/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.0912 - acc: 0.9819 - val_loss: 0.2312 - val_acc: 0.9396\n",
      "Epoch 396/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0908 - acc: 0.9822 - val_loss: 0.2347 - val_acc: 0.9408\n",
      "Epoch 397/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0905 - acc: 0.9821 - val_loss: 0.2279 - val_acc: 0.9403\n",
      "Epoch 398/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0900 - acc: 0.9824 - val_loss: 0.2293 - val_acc: 0.9396\n",
      "Epoch 399/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0897 - acc: 0.9824 - val_loss: 0.2461 - val_acc: 0.9387\n",
      "Epoch 400/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0897 - acc: 0.9827 - val_loss: 0.2277 - val_acc: 0.9415\n",
      "Epoch 401/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.0890 - acc: 0.9823 - val_loss: 0.2273 - val_acc: 0.9405\n",
      "Epoch 402/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.0889 - acc: 0.9826 - val_loss: 0.2273 - val_acc: 0.9398\n",
      "Epoch 403/500\n",
      "38444/38444 [==============================] - 8s 208us/step - loss: 0.0886 - acc: 0.9825 - val_loss: 0.2268 - val_acc: 0.9410\n",
      "Epoch 404/500\n",
      "38444/38444 [==============================] - 8s 209us/step - loss: 0.0883 - acc: 0.9825 - val_loss: 0.2262 - val_acc: 0.9410\n",
      "Epoch 405/500\n",
      "38444/38444 [==============================] - 7s 184us/step - loss: 0.0879 - acc: 0.9825 - val_loss: 0.2262 - val_acc: 0.9405\n",
      "Epoch 406/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.0875 - acc: 0.9830 - val_loss: 0.2274 - val_acc: 0.9408\n",
      "Epoch 407/500\n",
      "38444/38444 [==============================] - 7s 173us/step - loss: 0.0873 - acc: 0.9831 - val_loss: 0.2256 - val_acc: 0.9412\n",
      "Epoch 408/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0870 - acc: 0.9832 - val_loss: 0.2302 - val_acc: 0.9426\n",
      "Epoch 409/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0866 - acc: 0.9829 - val_loss: 0.2375 - val_acc: 0.9415\n",
      "Epoch 410/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0866 - acc: 0.9830 - val_loss: 0.2272 - val_acc: 0.9408\n",
      "Epoch 411/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0860 - acc: 0.9834 - val_loss: 0.2336 - val_acc: 0.9401\n",
      "Epoch 412/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0857 - acc: 0.9834 - val_loss: 0.2290 - val_acc: 0.9394\n",
      "Epoch 413/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0853 - acc: 0.9836 - val_loss: 0.2345 - val_acc: 0.9394\n",
      "Epoch 414/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0853 - acc: 0.9831 - val_loss: 0.2253 - val_acc: 0.9422\n",
      "Epoch 415/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.0845 - acc: 0.9834 - val_loss: 0.2270 - val_acc: 0.9417\n",
      "Epoch 416/500\n",
      "38444/38444 [==============================] - 7s 182us/step - loss: 0.0844 - acc: 0.9837 - val_loss: 0.2308 - val_acc: 0.9417\n",
      "Epoch 417/500\n",
      "38444/38444 [==============================] - 7s 180us/step - loss: 0.0843 - acc: 0.9838 - val_loss: 0.2272 - val_acc: 0.9410\n",
      "Epoch 418/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.0837 - acc: 0.9837 - val_loss: 0.2281 - val_acc: 0.9394\n",
      "Epoch 419/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0835 - acc: 0.9843 - val_loss: 0.2316 - val_acc: 0.9403\n",
      "Epoch 420/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.0835 - acc: 0.9838 - val_loss: 0.2423 - val_acc: 0.9382\n",
      "Epoch 421/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0834 - acc: 0.9841 - val_loss: 0.2277 - val_acc: 0.9419\n",
      "Epoch 422/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0826 - acc: 0.9840 - val_loss: 0.2276 - val_acc: 0.9405\n",
      "Epoch 423/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0823 - acc: 0.9842 - val_loss: 0.2251 - val_acc: 0.9417\n",
      "Epoch 424/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0821 - acc: 0.9841 - val_loss: 0.2318 - val_acc: 0.9408\n",
      "Epoch 425/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0820 - acc: 0.9841 - val_loss: 0.2306 - val_acc: 0.9419\n",
      "Epoch 426/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0814 - acc: 0.9844 - val_loss: 0.2268 - val_acc: 0.9408\n",
      "Epoch 427/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0810 - acc: 0.9842 - val_loss: 0.2260 - val_acc: 0.9417\n",
      "Epoch 428/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0810 - acc: 0.9842 - val_loss: 0.2289 - val_acc: 0.9419\n",
      "Epoch 429/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0806 - acc: 0.9845 - val_loss: 0.2299 - val_acc: 0.9396\n",
      "Epoch 430/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0803 - acc: 0.9847 - val_loss: 0.2365 - val_acc: 0.9396\n",
      "Epoch 431/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0805 - acc: 0.9844 - val_loss: 0.2287 - val_acc: 0.9412\n",
      "Epoch 432/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0799 - acc: 0.9847 - val_loss: 0.2307 - val_acc: 0.9419\n",
      "Epoch 433/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0796 - acc: 0.9849 - val_loss: 0.2257 - val_acc: 0.9419\n",
      "Epoch 434/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0793 - acc: 0.9849 - val_loss: 0.2281 - val_acc: 0.9410\n",
      "Epoch 435/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0788 - acc: 0.9850 - val_loss: 0.2261 - val_acc: 0.9426\n",
      "Epoch 436/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0787 - acc: 0.9851 - val_loss: 0.2263 - val_acc: 0.9415\n",
      "Epoch 437/500\n",
      "38444/38444 [==============================] - 7s 174us/step - loss: 0.0783 - acc: 0.9854 - val_loss: 0.2247 - val_acc: 0.9422\n",
      "Epoch 438/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0780 - acc: 0.9852 - val_loss: 0.2269 - val_acc: 0.9412\n",
      "Epoch 439/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0777 - acc: 0.9853 - val_loss: 0.2306 - val_acc: 0.9391\n",
      "Epoch 440/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0775 - acc: 0.9856 - val_loss: 0.2273 - val_acc: 0.9410\n",
      "Epoch 441/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0773 - acc: 0.9857 - val_loss: 0.2263 - val_acc: 0.9401\n",
      "Epoch 442/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0769 - acc: 0.9858 - val_loss: 0.2285 - val_acc: 0.9405\n",
      "Epoch 443/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0766 - acc: 0.9856 - val_loss: 0.2247 - val_acc: 0.9412\n",
      "Epoch 444/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0764 - acc: 0.9857 - val_loss: 0.2255 - val_acc: 0.9426\n",
      "Epoch 445/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0760 - acc: 0.9860 - val_loss: 0.2261 - val_acc: 0.9410\n",
      "Epoch 446/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.0757 - acc: 0.9860 - val_loss: 0.2275 - val_acc: 0.9412\n",
      "Epoch 447/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0754 - acc: 0.9861 - val_loss: 0.2272 - val_acc: 0.9415\n",
      "Epoch 448/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0754 - acc: 0.9861 - val_loss: 0.2268 - val_acc: 0.9422\n",
      "Epoch 449/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0750 - acc: 0.9863 - val_loss: 0.2262 - val_acc: 0.9419\n",
      "Epoch 450/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0747 - acc: 0.9865 - val_loss: 0.2282 - val_acc: 0.9415\n",
      "Epoch 451/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0745 - acc: 0.9862 - val_loss: 0.2263 - val_acc: 0.9403\n",
      "Epoch 452/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0742 - acc: 0.9866 - val_loss: 0.2259 - val_acc: 0.9417\n",
      "Epoch 453/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0740 - acc: 0.9868 - val_loss: 0.2283 - val_acc: 0.9401\n",
      "Epoch 454/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0737 - acc: 0.9868 - val_loss: 0.2275 - val_acc: 0.9401\n",
      "Epoch 455/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0735 - acc: 0.9864 - val_loss: 0.2320 - val_acc: 0.9412\n",
      "Epoch 456/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0731 - acc: 0.9873 - val_loss: 0.2341 - val_acc: 0.9401\n",
      "Epoch 457/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0730 - acc: 0.9867 - val_loss: 0.2396 - val_acc: 0.9368\n",
      "Epoch 458/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0730 - acc: 0.9867 - val_loss: 0.2286 - val_acc: 0.9417\n",
      "Epoch 459/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0725 - acc: 0.9873 - val_loss: 0.2273 - val_acc: 0.9426\n",
      "Epoch 460/500\n",
      "38444/38444 [==============================] - 7s 175us/step - loss: 0.0724 - acc: 0.9869 - val_loss: 0.2273 - val_acc: 0.9415\n",
      "Epoch 461/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.0719 - acc: 0.9872 - val_loss: 0.2263 - val_acc: 0.9405\n",
      "Epoch 462/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0717 - acc: 0.9872 - val_loss: 0.2263 - val_acc: 0.9403\n",
      "Epoch 463/500\n",
      "38444/38444 [==============================] - 7s 181us/step - loss: 0.0714 - acc: 0.9873 - val_loss: 0.2288 - val_acc: 0.9403\n",
      "Epoch 464/500\n",
      "38444/38444 [==============================] - 7s 183us/step - loss: 0.0712 - acc: 0.9875 - val_loss: 0.2283 - val_acc: 0.9410s \n",
      "Epoch 465/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0709 - acc: 0.9874 - val_loss: 0.2261 - val_acc: 0.9405\n",
      "Epoch 466/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.0706 - acc: 0.9875 - val_loss: 0.2279 - val_acc: 0.9431\n",
      "Epoch 467/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0706 - acc: 0.9874 - val_loss: 0.2267 - val_acc: 0.9422\n",
      "Epoch 468/500\n",
      "38444/38444 [==============================] - 7s 180us/step - loss: 0.0701 - acc: 0.9876 - val_loss: 0.2268 - val_acc: 0.9417\n",
      "Epoch 469/500\n",
      "38444/38444 [==============================] - 7s 179us/step - loss: 0.0698 - acc: 0.9878 - val_loss: 0.2285 - val_acc: 0.9412\n",
      "Epoch 470/500\n",
      "38444/38444 [==============================] - 7s 179us/step - loss: 0.0697 - acc: 0.9878 - val_loss: 0.2251 - val_acc: 0.9434\n",
      "Epoch 471/500\n",
      "38444/38444 [==============================] - 7s 179us/step - loss: 0.0693 - acc: 0.9878 - val_loss: 0.2262 - val_acc: 0.9408\n",
      "Epoch 472/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0693 - acc: 0.9877 - val_loss: 0.2266 - val_acc: 0.9415\n",
      "Epoch 473/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0688 - acc: 0.9881 - val_loss: 0.2284 - val_acc: 0.9403\n",
      "Epoch 474/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0687 - acc: 0.9879 - val_loss: 0.2260 - val_acc: 0.9419\n",
      "Epoch 475/500\n",
      "38444/38444 [==============================] - 7s 179us/step - loss: 0.0684 - acc: 0.9882 - val_loss: 0.2268 - val_acc: 0.9408\n",
      "Epoch 476/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0681 - acc: 0.9882 - val_loss: 0.2285 - val_acc: 0.9412\n",
      "Epoch 477/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0679 - acc: 0.9883 - val_loss: 0.2251 - val_acc: 0.9431\n",
      "Epoch 478/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0676 - acc: 0.9880 - val_loss: 0.2313 - val_acc: 0.9391\n",
      "Epoch 479/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0675 - acc: 0.9884 - val_loss: 0.2291 - val_acc: 0.9401\n",
      "Epoch 480/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0672 - acc: 0.9881 - val_loss: 0.2270 - val_acc: 0.9417\n",
      "Epoch 481/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0669 - acc: 0.9886 - val_loss: 0.2293 - val_acc: 0.9412\n",
      "Epoch 482/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0668 - acc: 0.9884 - val_loss: 0.2308 - val_acc: 0.9396\n",
      "Epoch 483/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0667 - acc: 0.9885 - val_loss: 0.2498 - val_acc: 0.9366\n",
      "Epoch 484/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0669 - acc: 0.9884 - val_loss: 0.2283 - val_acc: 0.9408\n",
      "Epoch 485/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0660 - acc: 0.9885 - val_loss: 0.2250 - val_acc: 0.9410\n",
      "Epoch 486/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0659 - acc: 0.9887 - val_loss: 0.2281 - val_acc: 0.9410\n",
      "Epoch 487/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0656 - acc: 0.9887 - val_loss: 0.2266 - val_acc: 0.9419\n",
      "Epoch 488/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0655 - acc: 0.9890 - val_loss: 0.2260 - val_acc: 0.9417\n",
      "Epoch 489/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0651 - acc: 0.9890 - val_loss: 0.2270 - val_acc: 0.9408\n",
      "Epoch 490/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0649 - acc: 0.9888 - val_loss: 0.2337 - val_acc: 0.9391\n",
      "Epoch 491/500\n",
      "38444/38444 [==============================] - 7s 179us/step - loss: 0.0646 - acc: 0.9892 - val_loss: 0.2273 - val_acc: 0.9401\n",
      "Epoch 492/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0647 - acc: 0.9893 - val_loss: 0.2248 - val_acc: 0.9417\n",
      "Epoch 493/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0641 - acc: 0.9893 - val_loss: 0.2251 - val_acc: 0.9419\n",
      "Epoch 494/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0640 - acc: 0.9891 - val_loss: 0.2454 - val_acc: 0.9368\n",
      "Epoch 495/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0641 - acc: 0.9891 - val_loss: 0.2314 - val_acc: 0.9380\n",
      "Epoch 496/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0635 - acc: 0.9893 - val_loss: 0.2251 - val_acc: 0.9441\n",
      "Epoch 497/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0632 - acc: 0.9896 - val_loss: 0.2266 - val_acc: 0.9408\n",
      "Epoch 498/500\n",
      "38444/38444 [==============================] - 7s 177us/step - loss: 0.0628 - acc: 0.9895 - val_loss: 0.2278 - val_acc: 0.9419\n",
      "Epoch 499/500\n",
      "38444/38444 [==============================] - 7s 176us/step - loss: 0.0628 - acc: 0.9895 - val_loss: 0.2249 - val_acc: 0.9424\n",
      "Epoch 500/500\n",
      "38444/38444 [==============================] - 7s 178us/step - loss: 0.0626 - acc: 0.9896 - val_loss: 0.2316 - val_acc: 0.9384\n",
      "Training completed on 03/31\n"
     ]
    }
   ],
   "source": [
    "# Fit dataset\n",
    "epochs = 500\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "model.save(model_dir+'hasyv2model51.h5')\n",
    "print(\"Training completed on 03/31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load model use\n",
    "model = load_model(model_dir+'hasyv2model51.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions from one hot encoded to label value\n",
    "y_pred = convert_pred_list_ohe_to_labels(res, threshold=0.1, get_max=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert test values from one hot encoded to label value\n",
    "y_true = convert_pred_list_ohe_to_labels(y_test, threshold=0.9, get_max=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x23d775c5e10>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x1440 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Spectral Color Map used: Violet means 1, Red means -1 and Yellow means 0\n",
    "cfs9 = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(20,20))\n",
    "sn.heatmap(cfs9, cmap=\"Spectral\", robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiclass Precision, Recall and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        16\n",
      "           1     1.0000    0.8333    0.9091         6\n",
      "           2     0.7692    0.7692    0.7692        13\n",
      "           3     1.0000    1.0000    1.0000         6\n",
      "           4     1.0000    0.8000    0.8889         5\n",
      "           5     1.0000    1.0000    1.0000         6\n",
      "           6     1.0000    1.0000    1.0000        12\n",
      "           7     1.0000    1.0000    1.0000         6\n",
      "           8     0.9091    1.0000    0.9524        10\n",
      "           9     1.0000    1.0000    1.0000        10\n",
      "          10     1.0000    1.0000    1.0000        10\n",
      "          11     1.0000    1.0000    1.0000        11\n",
      "          12     1.0000    1.0000    1.0000        12\n",
      "          13     1.0000    1.0000    1.0000        10\n",
      "          14     0.6364    0.8750    0.7368         8\n",
      "          15     1.0000    1.0000    1.0000         7\n",
      "          16     1.0000    1.0000    1.0000         7\n",
      "          17     1.0000    0.8750    0.9333         8\n",
      "          18     1.0000    1.0000    1.0000         6\n",
      "          19     1.0000    1.0000    1.0000         6\n",
      "          20     1.0000    0.8333    0.9091         6\n",
      "          21     0.8333    0.8333    0.8333         6\n",
      "          22     1.0000    1.0000    1.0000         6\n",
      "          23     1.0000    1.0000    1.0000         5\n",
      "          24     0.7143    0.8333    0.7692         6\n",
      "          25     0.8571    1.0000    0.9231         6\n",
      "          26     1.0000    0.7692    0.8696        13\n",
      "          27     1.0000    1.0000    1.0000        12\n",
      "          28     1.0000    1.0000    1.0000        12\n",
      "          29     1.0000    1.0000    1.0000        12\n",
      "          30     1.0000    1.0000    1.0000         6\n",
      "          31     0.8889    1.0000    0.9412         8\n",
      "          32     1.0000    1.0000    1.0000        10\n",
      "          33     1.0000    1.0000    1.0000         7\n",
      "          34     0.9167    0.9167    0.9167        12\n",
      "          35     0.8889    0.8889    0.8889         9\n",
      "          36     1.0000    0.9821    0.9910       112\n",
      "          37     0.9000    1.0000    0.9474         9\n",
      "          38     1.0000    1.0000    1.0000         6\n",
      "          39     0.7500    0.4286    0.5455         7\n",
      "          40     1.0000    0.8333    0.9091         6\n",
      "          41     0.8333    0.8333    0.8333         6\n",
      "          42     1.0000    0.8571    0.9231         7\n",
      "          43     1.0000    0.8333    0.9091         6\n",
      "          44     1.0000    1.0000    1.0000         6\n",
      "          45     1.0000    0.8333    0.9091         6\n",
      "          46     1.0000    1.0000    1.0000         5\n",
      "          47     1.0000    1.0000    1.0000         6\n",
      "          48     0.8000    0.6667    0.7273         6\n",
      "          49     1.0000    1.0000    1.0000         6\n",
      "          50     0.8000    0.6667    0.7273         6\n",
      "          51     0.7143    0.8333    0.7692         6\n",
      "          52     1.0000    1.0000    1.0000         5\n",
      "          53     1.0000    1.0000    1.0000         7\n",
      "          54     1.0000    1.0000    1.0000         6\n",
      "          55     0.8571    1.0000    0.9231         6\n",
      "          56     1.0000    1.0000    1.0000         6\n",
      "          57     1.0000    0.5000    0.6667         6\n",
      "          58     1.0000    1.0000    1.0000         6\n",
      "          59     0.5833    1.0000    0.7368         7\n",
      "          60     0.8000    0.6667    0.7273         6\n",
      "          61     1.0000    0.8333    0.9091         6\n",
      "          62     0.9845    0.9948    0.9896       191\n",
      "          63     0.9826    0.9912    0.9869       114\n",
      "          64     0.9920    0.9920    0.9920       125\n",
      "          65     1.0000    0.9800    0.9899       100\n",
      "          66     0.9855    0.9855    0.9855        69\n",
      "          67     1.0000    0.9846    0.9922        65\n",
      "          68     0.9464    0.9815    0.9636        54\n",
      "          69     0.9798    0.9700    0.9749       100\n",
      "          70     1.0000    1.0000    1.0000       115\n",
      "          71     0.9756    1.0000    0.9877        80\n",
      "          72     0.9714    0.9855    0.9784        69\n",
      "          73     0.9857    1.0000    0.9928        69\n",
      "          74     1.0000    1.0000    1.0000        63\n",
      "          75     1.0000    1.0000    1.0000        75\n",
      "          76     1.0000    0.9905    0.9952       105\n",
      "          77     0.9885    0.9885    0.9885        87\n",
      "          78     1.0000    1.0000    1.0000        11\n",
      "          79     0.8333    1.0000    0.9091        10\n",
      "          80     0.9487    0.9610    0.9548        77\n",
      "          81     1.0000    1.0000    1.0000        41\n",
      "          82     1.0000    1.0000    1.0000       158\n",
      "          83     0.9231    1.0000    0.9600        12\n",
      "          84     1.0000    1.0000    1.0000         9\n",
      "          85     0.9744    0.9744    0.9744        39\n",
      "          86     0.9911    0.9737    0.9823       114\n",
      "          87     1.0000    0.9688    0.9841        32\n",
      "          88     0.9769    0.9845    0.9807       129\n",
      "          89     0.9907    0.9907    0.9907       107\n",
      "          90     1.0000    1.0000    1.0000        89\n",
      "          91     0.9787    1.0000    0.9892        92\n",
      "          92     1.0000    0.9603    0.9797       151\n",
      "          93     0.9821    0.9821    0.9821        56\n",
      "          94     0.9706    1.0000    0.9851        33\n",
      "          95     0.9910    1.0000    0.9955       110\n",
      "          96     0.9909    0.9909    0.9909       110\n",
      "          97     0.9860    0.9907    0.9883       214\n",
      "          98     1.0000    0.9929    0.9964       140\n",
      "          99     0.9681    0.9891    0.9785        92\n",
      "         100     0.9630    1.0000    0.9811        26\n",
      "         101     0.9737    1.0000    0.9867        37\n",
      "         102     0.9966    0.9966    0.9966       291\n",
      "         103     0.9697    0.9697    0.9697        33\n",
      "\n",
      "    accuracy                         0.9822      4272\n",
      "   macro avg     0.9582    0.9477    0.9497      4272\n",
      "weighted avg     0.9831    0.9822    0.9821      4272\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cr9 = classification_report(y_true, y_pred, digits=4)\n",
    "print(cr9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 0., ..., 1., 1., 1.],\n",
       "         [0., 0., 0., ..., 1., 1., 1.],\n",
       "         [0., 0., 0., ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 1., 1., 0.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.]]],\n",
       "\n",
       "\n",
       "       [[[1., 1., 1., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         [1., 1., 1., ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 1., 1., 1.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 1., 1., ..., 0., 0., 0.]]]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
