{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line is required to run Tensorflow Object Detection API on Windows\n",
    "\n",
    "Set PYTHONPATH to two modules within the API to prevent any import errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!set PYTHONPATH=D:\\SharedLinux_D\\CPSC_597\\OCR_Math_Expressions\\models\\research;D:\\SharedLinux_D\\CPSC_597\\OCR_Math_Expressions\\models\\research\\slim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# call this before import maptlotlib ad after too\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "# import six.moves.urllib as urllib\n",
    "import sys\n",
    "# import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "import cv2\n",
    "import time\n",
    "# from distutils.version import StrictVersion\n",
    "# from collections import defaultdict\n",
    "# from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "import xml.etree.ElementTree as ET\n",
    "from utils import one_hot_encode_to_char_list, read_annotation, get_iou, calculate_map\n",
    "K.set_image_data_format('channels_first')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off tensorflow INFO (Debugging info)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OD_PATH = 'models/research/object_detection/'\n",
    "\n",
    "# append object_detection dir\n",
    "sys.path.append(OD_PATH+\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "# od imports\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object Detection Helpers\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape((im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "              'num_detections', 'detection_boxes', 'detection_scores',\n",
    "              'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                 feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "              'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Faster-RCNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VARIABLES\n",
    "MODEL_NAME = 'faster_rcnn_resnet50_inference_graph'\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_FROZEN_GRAPH = OD_PATH +'faster_rcnn_resnet50_training/'+ MODEL_NAME + '/frozen_inference_graph.pb'\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = OD_PATH + \"faster_rcnn_resnet50_training/data/object-detection.pbtxt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tf model\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_FROZEN_GRAPH, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Object Detection label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS, use_display_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Image paths: \n",
      "datasets/object_detection/test\\exp0015.png\n",
      "datasets/object_detection/test\\exp0016.png\n",
      "datasets/object_detection/test\\exp0029.png\n",
      "datasets/object_detection/test\\exp0037.png\n",
      "datasets/object_detection/test\\exp0044.png\n",
      "datasets/object_detection/test\\exp0051.png\n",
      "datasets/object_detection/test\\exp9.png\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_TEST_IMAGES_DIR = \"datasets/object_detection/test/*.png\"\n",
    "TEST_IMAGE_PATHS = list(glob.glob(PATH_TO_TEST_IMAGES_DIR))\n",
    "print(\"Test Image paths: \")\n",
    "for i in TEST_IMAGE_PATHS:\n",
    "    print(i)\n",
    "    \n",
    "# Change TEST/TRAIN Images to test here\n",
    "image_paths = TEST_IMAGE_PATHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load CNN/ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = (320, 320)\n",
    "models_path = \"trained_models/\"\n",
    "\n",
    "# CNN\n",
    "cnn = load_model(models_path+\"model3.h5\")\n",
    "\n",
    "# ANN\n",
    "#ann = load_model(models_path+\"ann1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latex</th>\n",
       "      <th>old_symbol</th>\n",
       "      <th>new_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  latex  old_symbol  new_id\n",
       "0     A          31       0\n",
       "1     B          32       1\n",
       "2     C          33       2\n",
       "3     D          34       3\n",
       "4     E          35       4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols = pd.read_csv(\"processed_data/symbols.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "symbols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [TEST_IMAGE_PATHS[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------\n",
      "Detecting Boxes for Image:  datasets/object_detection/test/exp0016.png\n",
      "Image shape:  (320, 320, 3)\n",
      "***************\n",
      "Box: \n",
      "ymin:  0.4592886  xmin:  0.80772877  ymax:  0.58294106  xmax  0.8950295\n",
      "left:  258.47320556640625  right:  286.4094352722168  top:  146.97235107421875  bottom  186.5411376953125\n",
      "Top left point: ( 258 ,  146 ) and Bottom right point: ( 286 ,  186 )\n",
      "CNN/ANN Predictions:\n",
      "Symbol_id: 60 , Latex: y , Confidence Score: 65.06 %\n",
      "Symbol_id: 24 , Latex: Y , Confidence Score: 12.61 %\n",
      "Symbol_id: 59 , Latex: x , Confidence Score: 6.84 %\n",
      "Symbol_id: 63 , Latex: \\gamma , Confidence Score: 3.63 %\n",
      "Symbol_id: 61 , Latex: z , Confidence Score: 3.46 %\n",
      "Symbol_id: 33 , Latex: 7 , Confidence Score: 2.49 %\n",
      "Symbol_id: 25 , Latex: Z , Confidence Score: 1.46 %\n",
      "Symbol_id: 92 , Latex: \\times , Confidence Score: 1.29 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAADQAAABECAYAAADOWhYqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAABYElEQVRoge2a0Q7DIAhF67L//2X2ZOKMFbXAJZTztqwl3l7AFFuI6IrEB70AaVKQd1KQd77M/25aYCnl7zcRldF1U4f6IF6YbTWvS7k/lxCb8G6WsILugluIG9QNe89UUBugD3735LSErsZ9Xw1VZm61nKTJSpxVtmqoMlskl5q7Anevf2/KrdI/0V3Hnm7mhbFUvGXtLJhZ2zCQuEMcq83ltJmEqyGoICIS34jNU65Hat+quEo5CbdcCZIAKkjjBRJSQ9J105IpJ4102zYXpD14MauhkRCNt1t4ykkD6XKaAxYThywHluqCrKevWUOnWE1dwzkUTpBayqGOYkwcsjy1UBGEPCgLV0PqgqwPydIhDvRBcziH1PYh1FdeYoLQqVYJl3IqgpAfFaZDI7zUz3UFdOhRl7Oate1w7JCnNGs5dgjtxB3haigFeScFeYfrcj5784RwDqUg76Qg74QT9AN0YGul58WKigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "Box: \n",
      "ymin:  0.4637743  xmin:  0.050667226  ymax:  0.58251876  xmax  0.17326547\n",
      "left:  16.213512420654297  right:  55.44495105743408  top:  148.40777397155762  bottom  186.40600204467773\n",
      "Top left point: ( 16 ,  148 ) and Bottom right point: ( 55 ,  186 )\n",
      "CNN/ANN Predictions:\n",
      "Symbol_id: 25 , Latex: Z , Confidence Score: 93.33 %\n",
      "Symbol_id: 61 , Latex: z , Confidence Score: 6.22 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEYAAABECAYAAAA85kOPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAABiElEQVR4nO2byw7CMAwEE8T//3I4IVnANnWxE9vduVY8Otp1AoQ+xmjkm8fuNxAVigFQDIBiABQDeE6uV1+yOrpwKKZ3+LgtrNxasEqAWZVCcTbBFsnqkycJMWP+qfTk/uATs0qAFInR8CtdB/d4n8SMMUxmTDkxVpQUY7H/KifmU8rVWpUTY0WqDd4RVkl5UyIx1lJaS54Y5Z5FRYnEeJA2MR71kaQT41kfCasESJcYiec3eqkSs/Kr1lRiVpKiSqsGriS8GO9lGcEqAUImBg3Zlb8rhRAzW212nOHZJubM0rvzUBNnDGBLYnYsv1qWickgQ8IqAVwTczRgI6elNQcxmWVIWCWASWKqpERyWUxFGRJWCaBKTPWUSE6LifCJdyWsEmCamGxbeStUB6DvIOSNqkq993Cnxb3gjAFc2uDdYdlWnfO1rlEAifc552uF2cnwjEN5jHHt/0rKF1E/JrJMVgmw9Qe3AMMXwsQAKAZAMYDZjIm7bDjDxAAoBkAxAIoBUAyAYgAv2bF3rAQb1zMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "Box: \n",
      "ymin:  0.4696555  xmin:  0.21608396  ymax:  0.5887567  xmax  0.3260028\n",
      "left:  69.14686679840088  right:  104.32089805603027  top:  150.28976440429688  bottom  188.40213775634766\n",
      "Top left point: ( 69 ,  150 ) and Bottom right point: ( 104 ,  188 )\n",
      "CNN/ANN Predictions:\n",
      "Symbol_id: 83 , Latex: - , Confidence Score: 48.59 %\n",
      "Symbol_id: 104 , Latex: = , Confidence Score: 22.36 %\n",
      "Symbol_id: 94 , Latex: \\div , Confidence Score: 14.23 %\n",
      "Symbol_id: 75 , Latex: \\cdot , Confidence Score: 7.81 %\n",
      "Symbol_id: 96 , Latex: \\perp , Confidence Score: 4.4 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABECAYAAAAx+DPIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAA7UlEQVR4nO3ZMQ7CMBAFUS/i/lcOHUoRySbNw9k/ZaCYTNYiWuo4jtGZlxbQJIAW0CSAFtC8J58/6Seiri62n4AE0AKaBNACmgTQApoE0AKa2Zvgl6rLF6m/4s5uo/0ETANU1RZPf4x7Uzo9Ak9fmeUIaAFNAmgBTQJoAU0CaAFNAmgBTQJoAU0CaAFN+wDLK7Ex9liLnVnZZSxPwG43P8baNitHYPWLT12NtZ+ABNACmgTQApoE0AKaBNACmgTQApoE0AKaBNACmgTQApoE0AKaBNACmvYBZmvx/f4N+ZH2E5AAWkCTAFpA0z7AB3vIEJ6OTlK7AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "Box: \n",
      "ymin:  0.46143577  xmin:  0.58968365  ymax:  0.5513729  xmax  0.7266938\n",
      "left:  188.69876861572266  right:  232.54201889038086  top:  147.65944480895996  bottom  176.43932342529297\n",
      "Top left point: ( 188 ,  147 ) and Bottom right point: ( 232 ,  176 )\n",
      "CNN/ANN Predictions:\n",
      "Symbol_id: 96 , Latex: \\perp , Confidence Score: 31.84 %\n",
      "Symbol_id: 19 , Latex: T , Confidence Score: 22.73 %\n",
      "Symbol_id: 84 , Latex: + , Confidence Score: 16.82 %\n",
      "Symbol_id: 71 , Latex: \\Pi , Confidence Score: 16.27 %\n",
      "Symbol_id: 94 , Latex: \\div , Confidence Score: 3.13 %\n",
      "Symbol_id: 5 , Latex: F , Confidence Score: 2.52 %\n",
      "Symbol_id: 36 , Latex: \\sigma , Confidence Score: 1.46 %\n",
      "Symbol_id: 89 , Latex: \\# , Confidence Score: 1.32 %\n",
      "Symbol_id: 97 , Latex: \\forall , Confidence Score: 1.02 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEYAAAAxCAYAAABnCd/9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAA6ElEQVRoge3YwQ6CMBAGYWt8/1deT0SCGWoCdv/DfFcONsPSgqOqHvr27F5AKsMAwwDDAMOA1+T6X4+sMcbnh3pOx0EXYiZmHylBTJg0rWGSXy6jJibpcWoPU1UtkzO7CbNTabltwVdi3TF57ROTKmZiquqnO71qH4oJc3Q1wNV9axom6aTYrNis3WPA6cR0T0vnC+BpmNUL674Re1GPUtInQlSYJIYBhgGGAYYBhgHRYTrfa+LCdP1xdRQXJoVhgGGAYYBhgGGAYYBhgGGAYYBhgGGAYcBI+JJN5MQAwwDDAMMAwwDDgDdSTS5p+UuVpAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***************\n",
      "Box: \n",
      "ymin:  0.46859694  xmin:  0.39668804  ymax:  0.5685658  xmax  0.5022097\n",
      "left:  126.9401741027832  right:  160.70711135864258  top:  149.95101928710938  bottom  181.9410514831543\n",
      "Top left point: ( 126 ,  149 ) and Bottom right point: ( 160 ,  181 )\n",
      "CNN/ANN Predictions:\n",
      "Symbol_id: 92 , Latex: \\times , Confidence Score: 99.85 %\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEYAAABCCAYAAADqv6CSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAABu0lEQVR4nO2a24rDMAxEldL//+X0pYGQeGzL0cUOc2BhacLaOx5Jlui277uQO5/sDcwKhQFQGACFAVAYwLfxfMmStW3b7TNQfe8v/qFjAC3HLEPJJSLQKU2WFwYJIjIuighDCbK0Y65usWxv6BhAl2MU5S+EWl6xoipMbQPnZ1EiRR4QQwlQdUzpNEqnluEe77XoGIC6XJ9PKcI9EYm2uG5j86r/zPJa7nWjvS6DHjCUAKY3XxRmx+9PTjr63kTHANyEQaX++KmRlXDPuDaRV3FWEOSAoQRIGzucE/JsTaoIHQMJFaa398p2i0hCKGkTchYMJUCaMLX7zAwuCg+lWk4ptRGl9yJgKAFCHdMKEc2s5/q+NXQMwHRQBRcxyhcOeQda2FUYrwRqOCnkBE9Leksw+ndq8x4L6BiAW7mOuL16zpjNk2/mjXVgbSZfLaEzX29qPZd2L3QMwFSYGcYFIuVyri3lJsKUFp1hPNk7Si3BUAI8Sr7Zw6QeRvdDxwCGhVnBLU/oDqWgL/JMA0MJMPwFaJF3OuWAjgF0OebNzkDQMQAKA2iF0hxdYQJ0DIDCACgMgMIAKAyAwgB+/WWwtJli7EoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true {'x1': 14, 'x2': 62, 'y1': 148, 'y2': 186}\n",
      "pred {'x1': 16.213512420654297, 'x2': 55.44495105743408, 'y1': 148.40777397155762, 'y2': 186.40600204467773}\n",
      "IOU : 0.8015515135072201\n",
      "---------------\n",
      "true {'x1': 126, 'x2': 161, 'y1': 148, 'y2': 182}\n",
      "pred {'x1': 126.9401741027832, 'x2': 160.70711135864258, 'y1': 149.95101928710938, 'y2': 181.9410514831543}\n",
      "IOU : 0.9077356386359413\n",
      "---------------\n",
      "true {'x1': 188, 'x2': 235, 'y1': 144, 'y2': 178}\n",
      "pred {'x1': 188.69876861572266, 'x2': 232.54201889038086, 'y1': 147.65944480895996, 'y2': 176.43932342529297}\n",
      "IOU : 0.7896141558511715\n",
      "---------------\n",
      "true {'x1': 251, 'x2': 288, 'y1': 144, 'y2': 184}\n",
      "pred {'x1': 258.47320556640625, 'x2': 286.4094352722168, 'y1': 146.97235107421875, 'y2': 186.5411376953125}\n",
      "IOU : 0.6669372690962341\n",
      "---------------\n",
      "true {'x1': 70, 'x2': 103, 'y1': 148, 'y2': 184}\n",
      "pred {'x1': 69.14686679840088, 'x2': 104.32089805603027, 'y1': 150.28976440429688, 'y2': 188.40213775634766}\n",
      "IOU : 0.785548866803322\n",
      "---------------\n",
      "{'TP': 5, 'FP': 0, 'TN': 0, 'FN': 0}\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Average Precision (mAP):  1.0\n",
      "Mean IOU:  0.7902774887787778\n"
     ]
    }
   ],
   "source": [
    "cnf = {'TP':0, 'FP':0, 'TN':0, 'FN':0}\n",
    "map_data = []\n",
    "iou_data = []\n",
    "\n",
    "for image_path in image_paths:\n",
    "    image_path = image_path.replace(\"\\\\\", \"/\")\n",
    "    im_name = image_path.split(\"/\")[-1]\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = np.asarray(image)\n",
    "    if len(image_np.shape) < 3:\n",
    "    \timage_np = cv2.cvtColor(image_np, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "    \timage_np = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)\n",
    "    print('-'*25)\n",
    "    print('Detecting Boxes for Image: ', image_path)\n",
    "    \n",
    "    print('Image shape: ', image_np.shape)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    # crop image and display\n",
    "    im_width, im_height = image.size\n",
    "    cropped_images = []\n",
    "    boxes_pred = []\n",
    "    for i in range(output_dict['num_detections']):\n",
    "        if output_dict['detection_scores'][i] > 0.8:\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np,\n",
    "                np.array([output_dict['detection_boxes'][i]]),\n",
    "                np.array([output_dict['detection_classes'][i]]),\n",
    "                np.array([output_dict['detection_scores'][i]]),\n",
    "                category_index,\n",
    "                instance_masks=output_dict.get('detection_masks'),\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=1)\n",
    "            (ymin, xmin, ymax, xmax) = output_dict['detection_boxes'][i]\n",
    "            (left, right, top, bottom) = (xmin * im_width,\n",
    "                                          xmax * im_width,\n",
    "                                          ymin * im_height,\n",
    "                                          ymax * im_height)\n",
    "            print('*'*15)\n",
    "            print(\"Box: \")\n",
    "            print(\"ymin: \", ymin,\" xmin: \", xmin, \" ymax: \", ymax,\" xmax \", xmax)\n",
    "            print(\"left: \", left,\" right: \", right,\" top: \", top,\" bottom \", bottom)\n",
    "            print(\"Top left point: (\",int(left),\", \",int(top),\") and Bottom right point: (\",int(right),\", \",int(bottom),\")\")\n",
    "            \n",
    "            cropped_image = image.crop((left, top, right, bottom))\n",
    "            \n",
    "            \n",
    "            # boxes_pred.append([left, right, top, bottom])\n",
    "            boxes_pred.append({\n",
    "                'x1':left,\n",
    "                'x2':right,\n",
    "                'y1':top,\n",
    "                'y2':bottom\n",
    "            })\n",
    "            \n",
    "            # Predict label for cropped image with CNN\n",
    "            c_im = np.asarray(cropped_image)\n",
    "            c_im = cv2.cvtColor(c_im, cv2.COLOR_BGRA2RGBA)\n",
    "            c_im = cv2.cvtColor(c_im, cv2.COLOR_RGBA2RGB)\n",
    "            c_im = cv2.cvtColor(c_im, cv2.COLOR_RGBA2GRAY)\n",
    "            c_im = cv2.resize(c_im, (32,32))\n",
    "            c_im = c_im / 255.0\n",
    "            # Symbol Prediction\n",
    "            # CNN\n",
    "            c_im = np.resize(c_im, (1, 1, 32, 32))\n",
    "            res = cnn.predict(c_im)\n",
    "            # ANN\n",
    "            # c_im = c_im.flatten().reshape((1, 32*32))\n",
    "            # print(c_im.shape)\n",
    "            # res = ann.predict(c_im)\n",
    "            #\n",
    "            res = res.flatten()\n",
    "            lbls = one_hot_encode_to_char_list(res, threshold=0.01, get_max = False)\n",
    "            print(\"CNN/ANN Predictions:\")\n",
    "            for j in lbls:\n",
    "                symbols_row = symbols[symbols['new_id'] == j[0]][['latex', 'old_symbol', 'new_id']]\n",
    "                latex, old_symbol, new_id = symbols_row.iloc[0].to_list()\n",
    "                print('Symbol_id:', new_id, ', Latex:', latex, ', Confidence Score:', round(j[1]*100, 2), '%')\n",
    "            plt.figure(figsize=(1,1))\n",
    "            plt.axis('off')\n",
    "            temp_ = np.asarray(cropped_image)\n",
    "            plt.imshow(temp_)\n",
    "            plt.show()\n",
    "            cropped_images.append(np.asarray(cropped_image))\n",
    "            # cv2.imshow('model', np.asarray(cropped_image))\n",
    "    # calculate IOU\n",
    "    xml_path = image_path[:-3]+'xml'\n",
    "    xml_path = xml_path.replace(\"\\\\\", \"/\")\n",
    "    boxes_true = read_annotation(xml_path)\n",
    "\n",
    "    ious = []\n",
    "    iou_threshold = 0.5\n",
    "    # dont reset cnf\n",
    "    cnf = {'TP':0, 'FP':0, 'TN':0, 'FN':0}\n",
    "    for t in boxes_true:\n",
    "        box_present = False\n",
    "        for p in boxes_pred:\n",
    "            if p['x1'] <= t['x1'] + 10 and p['x1'] >= t['x1'] - 10:\n",
    "                print('true', t)\n",
    "                print('pred', p)\n",
    "                iou = get_iou(t, p)\n",
    "                iou_data.append(iou)\n",
    "                ious.append(iou)\n",
    "                print('IOU :', iou)\n",
    "                print('-----'*3)\n",
    "                box_present = True\n",
    "                if iou > iou_threshold:\n",
    "                    cnf['TP'] += 1\n",
    "                else:\n",
    "                    cnf['FP'] += 1\n",
    "                break\n",
    "        if not box_present:\n",
    "            cnf['FN'] += 1\n",
    "    print(cnf)\n",
    "    \n",
    "    # prevent division by 0 error\n",
    "    if cnf['TP'] + cnf['FP'] != 0 and cnf['TP'] + cnf['FN'] != 0:\n",
    "        precision = cnf['TP'] / (cnf['TP'] + cnf['FP'])\n",
    "        print('Precision: ', precision)\n",
    "        recall = cnf['TP']/ (cnf['TP'] + cnf['FN'])\n",
    "        print('Recall: ', recall)\n",
    "        map_data.append((precision, recall))\n",
    "    #\n",
    "\n",
    "    plt.figure(figsize=(7,7))\n",
    "    plt.axis('off')\n",
    "    temp_ = cv2.resize(image_np, (int(im_width), int(im_height)))\n",
    "    plt.imshow(temp_)\n",
    "    plt.show()\n",
    "\n",
    "# Calculate mAP (Mean Average Precision Score)\n",
    "# mean_ap = calculate_map(map_data)\n",
    "# print(\"Mean Average Precision (mAP): \", mean_ap)\n",
    "print(\"Mean Average Precision (mAP): \", np.mean([i[0] for i in map_data]))\n",
    "\n",
    "# Mean IOUs (not an ideal metric)\n",
    "print(\"Mean IOU: \", np.mean(iou_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
