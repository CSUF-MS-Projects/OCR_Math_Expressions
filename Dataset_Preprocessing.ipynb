{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DO NOT RUN ALL CELLS, SOME CELLS SAVE FILES. THIS MAY OVERWRITE PREVIOUS SPLITS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_ = 'HASYv2/'\n",
    "model_dir = 'trained_models/'\n",
    "data_dir = 'data/'\n",
    "processed_data_dir = 'processed_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ORIGINAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     path  symbol_id latex  user_id\n",
      "0  hasy-data/v2-00000.png         31     A       50\n",
      "1  hasy-data/v2-00001.png         31     A       10\n",
      "2  hasy-data/v2-00002.png         31     A       43\n",
      "3  hasy-data/v2-00003.png         31     A       43\n",
      "4  hasy-data/v2-00004.png         31     A     4435\n",
      "-----------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 168233 entries, 0 to 168232\n",
      "Data columns (total 4 columns):\n",
      "path         168233 non-null object\n",
      "symbol_id    168233 non-null int64\n",
      "latex        168233 non-null object\n",
      "user_id      168233 non-null int64\n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 5.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "orig_dataset = read_csv(dir_+'hasy-data-labels.csv')\n",
    "print(orig_dataset.head())\n",
    "print('-----------------')\n",
    "print(orig_dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get relevant rows from dataset\n",
    "up_alphas = [chr(i+65) for i in range(26)]\n",
    "lw_alphas = [chr(i+97) for i in range(26)]\n",
    "nums = [str(n) for n in range(10)]\n",
    "ops =  ['=','+','-','\\\\%','*','(',')',r'\\{', r'\\}', r'[', r']','\\\\times','\\\\div','\\\\ast','\\\\cup','\\\\cap','\\\\subset','\\\\subseteq','\\\\supset','\\\\|','\\\\perp','<','>','\\\\leq','\\\\geq']\n",
    "greeks = ['\\\\sigma', '\\\\Sigma', '\\\\gamma', '\\\\delta', '\\\\Delta', '\\\\eta', '\\\\theta', '\\\\epsilon', '\\\\lambda', '\\\\mu', '\\\\Pi', '\\\\rho', '\\\\phi', '\\\\omega', '\\\\ohm']\n",
    "specials = ['\\\\$','\\\\&','\\\\#','\\\\infty','\\\\exists','\\\\forall','\\\\cdot']\n",
    "relevant_labels = up_alphas + lw_alphas + nums + ops + greeks + specials\n",
    "whole_dataset = orig_dataset.loc[orig_dataset['latex'].isin(relevant_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M',\n",
       "       'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z',\n",
       "       '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '\\\\sigma', 'a',\n",
       "       'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n',\n",
       "       'o', 'p', 'q', 'r', 's', 'u', 'v', 'w', 'x', 'y', 'z', '\\\\Sigma',\n",
       "       '\\\\gamma', '\\\\delta', '\\\\Delta', '\\\\eta', '\\\\theta', '\\\\epsilon',\n",
       "       '\\\\lambda', '\\\\mu', '\\\\Pi', '\\\\rho', '\\\\phi', '\\\\omega', '\\\\cdot',\n",
       "       '\\\\leq', '\\\\geq', '<', '>', '\\\\subset', '\\\\supset', '\\\\subseteq',\n",
       "       '-', '+', '\\\\$', '\\\\{', '\\\\}', '\\\\&', '\\\\#', '\\\\%', '\\\\cup',\n",
       "       '\\\\times', '\\\\ast', '\\\\div', '\\\\cap', '\\\\perp', '\\\\forall',\n",
       "       '\\\\exists', '[', ']', '\\\\|', '\\\\infty', '\\\\ohm'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_dataset['latex'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This total should be 107, = in one\n",
    "len(whole_dataset['latex'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>symbol_id</th>\n",
       "      <th>latex</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hasy-data/v2-00000.png</td>\n",
       "      <td>31</td>\n",
       "      <td>A</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hasy-data/v2-00001.png</td>\n",
       "      <td>31</td>\n",
       "      <td>A</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hasy-data/v2-00002.png</td>\n",
       "      <td>31</td>\n",
       "      <td>A</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hasy-data/v2-00003.png</td>\n",
       "      <td>31</td>\n",
       "      <td>A</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hasy-data/v2-00004.png</td>\n",
       "      <td>31</td>\n",
       "      <td>A</td>\n",
       "      <td>4435</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     path  symbol_id latex  user_id\n",
       "0  hasy-data/v2-00000.png         31     A       50\n",
       "1  hasy-data/v2-00001.png         31     A       10\n",
       "2  hasy-data/v2-00002.png         31     A       43\n",
       "3  hasy-data/v2-00003.png         31     A       43\n",
       "4  hasy-data/v2-00004.png         31     A     4435"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels in custom dataset are  107\n"
     ]
    }
   ],
   "source": [
    "ltx_dict = {'equal':'=', 'lroundb':'//(', 'rroundb':'//)'}\n",
    "# function to get only alphabets from string\n",
    "getalpha = lambda x: \"\".join([c for c in x if c.isalpha()])\n",
    "\n",
    "# cosntruct a dataframe for custom_images\n",
    "path_iter = glob.iglob(\"HASYv2/custom-data/*\")\n",
    "custom_paths = [str(p).split('/')[-1] for p in path_iter]\n",
    "\n",
    "ltxs = [ltx_dict[getalpha(i.split('\\\\')[-1].split('.')[0])] for i in custom_paths]\n",
    "\n",
    "syms = {list(ltx_dict.values())[i]:10000+i for i in range(len(ltx_dict))}\n",
    "#syms ={v:i for i, v in enumerate(syms)}\n",
    "symbol_ids = [syms[i] for i in ltxs]\n",
    "user_ids = [7000 for i in range(len(ltxs))]\n",
    "\n",
    "custom_dataset = pd.DataFrame(data={\n",
    "    'path':custom_paths,\n",
    "    'symbol_id':symbol_ids,\n",
    "    'latex':ltxs,\n",
    "    'user_id':user_ids \n",
    "})\n",
    "\n",
    "# combine relevant and custom symbols\n",
    "whole_dataset = pd.concat([whole_dataset, custom_dataset], axis = 0).reset_index(drop=True)\n",
    "whole_dataset.drop(['user_id'], axis=1, inplace=True)\n",
    "print(\"Number of labels in custom dataset are \", len(whole_dataset['latex'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SYMBOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   symbol_id latex  training_samples  test_samples\n",
      "0         31     A               137            22\n",
      "1         32     B                53             8\n",
      "2         33     C               120            14\n",
      "3         34     D                50             8\n",
      "4         35     E                48             6\n",
      "-----------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 369 entries, 0 to 368\n",
      "Data columns (total 4 columns):\n",
      "symbol_id           369 non-null int64\n",
      "latex               369 non-null object\n",
      "training_samples    369 non-null int64\n",
      "test_samples        369 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 11.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read symbols for all classification classes\n",
    "symbols = read_csv(dir_+'symbols.csv')\n",
    "print(symbols.head())\n",
    "print('-----------------')\n",
    "print(symbols.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lebels in symbols df is  107\n"
     ]
    }
   ],
   "source": [
    "symbols = symbols.loc[symbols['latex'].isin(relevant_labels)]\n",
    "ltxs = list(ltx_dict.values())\n",
    "symbol_ids = [syms[i] for i in ltxs]\n",
    "train_count = {ltxs[i]:0 for i in range(len(ltxs))}\n",
    "test_count = {ltxs[i]:0 for i in range(len(ltxs))}\n",
    "for i in custom_paths:\n",
    "    key = getalpha(i.split('\\\\')[-1].split('.')[0])\n",
    "    train_count[ltx_dict[key]]+= 1\n",
    "train_count = [train_count[i] for i in ltxs]\n",
    "test_count = [test_count[i] for i in ltxs]\n",
    "custom_symbols = pd.DataFrame(data={\n",
    "    'symbol_id': symbol_ids,\n",
    "    'latex': ltxs,\n",
    "    'training_samples': train_count,\n",
    "    'test_samples': test_count\n",
    "})\n",
    "symbols = pd.concat([symbols, custom_symbols], axis=0).reset_index(drop=True)\n",
    "print(\"Number of lebels in symbols df is \", len(symbols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reset Symbol names to start from 0 and end at max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change name of previous symbol\n",
    "symbols['old_symbol'] = symbols['symbol_id']\n",
    "symbols = symbols.drop(['symbol_id', 'training_samples', 'test_samples'], axis=1)\n",
    "\n",
    "# add new id according to index of character\n",
    "symbols['new_id'] = symbols.index\n",
    "\n",
    "# make a symbols dict that references each symbol_id to the new symbol\n",
    "symbols_dict = {}\n",
    "for i in range(len(symbols)):\n",
    "    symbols_dict[symbols['old_symbol'][i]] = symbols['new_id'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dataset['symbol_id'] = [symbols_dict[i] for i in whole_dataset['symbol_id']]\n",
    "symbols_list = np.array(whole_dataset['symbol_id']).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbols.to_csv(processed_data_dir+'symbols.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN THIS CELL ONLY ONCE for creating the splits, \n",
    "# then load the saved csv's for both train and test splits\n",
    "train, test = get_df_split(whole_dataset, stratify_col='symbol_id', test_size=0.1)\n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "train.to_csv(processed_data_dir+'train.csv')\n",
    "test.to_csv(processed_data_dir+'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to load csvs\n",
    "train = read_csv(processed_data_dir+'train.csv').drop(['Unnamed: 0'], axis=1).reset_index(drop=True)\n",
    "test = read_csv(processed_data_dir+'test.csv').drop(['Unnamed: 0'], axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: (143, 16, 89.94), 1: (55, 6, 90.16), 2: (121, 13, 90.3), 3: (52, 6, 89.66), 4: (49, 5, 90.74), 5: (50, 6, 89.29), 6: (106, 12, 89.83), 7: (58, 6, 90.62), 8: (90, 10, 90.0), 9: (94, 10, 90.38), 10: (86, 10, 89.58), 11: (100, 11, 90.09), 12: (108, 12, 90.0), 13: (95, 10, 90.48), 14: (76, 8, 90.48), 15: (65, 7, 90.28), 16: (60, 7, 89.55), 17: (75, 8, 90.36), 18: (57, 6, 90.48), 19: (50, 6, 89.29), 20: (53, 6, 89.83), 21: (50, 6, 89.29), 22: (56, 6, 90.32), 23: (49, 5, 90.74), 24: (50, 6, 89.29), 25: (59, 6, 90.77), 26: (120, 13, 90.23), 27: (106, 12, 89.83), 28: (112, 12, 90.32), 29: (108, 12, 90.0), 30: (55, 6, 90.16), 31: (70, 8, 89.74), 32: (90, 10, 90.0), 33: (68, 7, 90.67), 34: (109, 12, 90.08), 35: (81, 9, 90.0), 36: (1011, 112, 90.03), 37: (77, 9, 89.53), 38: (51, 6, 89.47), 39: (60, 7, 89.55), 40: (52, 6, 89.66), 41: (57, 6, 90.48), 42: (59, 7, 89.39), 43: (50, 6, 89.29), 44: (52, 6, 89.66), 45: (53, 6, 89.83), 46: (48, 5, 90.57), 47: (52, 6, 89.66), 48: (50, 6, 89.29), 49: (51, 6, 89.47), 50: (55, 6, 90.16), 51: (56, 6, 90.32), 52: (50, 5, 90.91), 53: (62, 7, 89.86), 54: (55, 6, 90.16), 55: (53, 6, 89.83), 56: (51, 6, 89.47), 57: (57, 6, 90.48), 58: (50, 6, 89.29), 59: (59, 7, 89.39), 60: (52, 6, 89.66), 61: (54, 6, 90.0), 62: (1719, 191, 90.0), 63: (1026, 114, 90.0), 64: (1121, 125, 89.97), 65: (896, 100, 89.96), 66: (617, 69, 89.94), 67: (586, 65, 90.02), 68: (490, 54, 90.07), 69: (896, 100, 89.96), 70: (1032, 115, 89.97), 71: (719, 80, 89.99), 72: (623, 69, 90.03), 73: (618, 69, 89.96), 74: (568, 63, 90.02), 75: (680, 75, 90.07), 76: (949, 105, 90.04), 77: (779, 87, 89.95), 78: (103, 11, 90.35), 79: (91, 10, 90.1), 80: (695, 77, 90.03), 81: (370, 41, 90.02), 82: (1418, 158, 89.97), 83: (106, 12, 89.83), 84: (81, 9, 90.0), 85: (348, 39, 89.92), 86: (1022, 114, 89.96), 87: (289, 32, 90.03), 88: (1157, 129, 89.97), 89: (960, 107, 89.97), 90: (798, 89, 89.97), 91: (831, 92, 90.03), 92: (1358, 151, 89.99), 93: (501, 56, 89.95), 94: (302, 33, 90.15), 95: (988, 110, 89.98), 96: (991, 110, 90.01), 97: (1925, 214, 90.0), 98: (1262, 140, 90.01), 99: (824, 92, 89.96), 100: (233, 26, 89.96), 101: (333, 37, 90.0), 102: (2623, 291, 90.01), 103: (297, 33, 90.0), 104: (4, 0, 100.0), 105: (6, 0, 100.0), 106: (6, 0, 100.0)}\n"
     ]
    }
   ],
   "source": [
    "# (train_count, test_count, percentage of train count to total)\n",
    "labels_count = get_label_count_df(train, test, symbols_list)\n",
    "print(labels_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One Hot Encode Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of dataset 38444\n",
      "len of dataset 4272\n"
     ]
    }
   ],
   "source": [
    "no_categories = len(symbols)\n",
    "train_one_hot_symbols = convert_to_one_hot_encode(train['symbol_id'], no_categories)\n",
    "test_one_hot_symbols = convert_to_one_hot_encode(test['symbol_id'], no_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole_dataset['symbol_id_ohe'] = [list(one_hot_symbols[i]) for i in range(len(whole_dataset))]\n",
    "train['symbol_id_ohe'] = [list(train_one_hot_symbols[i]) for i in range(len(train))]\n",
    "test['symbol_id_ohe'] = [list(test_one_hot_symbols[i]) for i in range(len(test))]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
