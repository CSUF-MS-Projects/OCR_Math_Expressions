{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------CLASS NAMES-------------------\n",
      "['+', '-', '0', '2', '=', 'A', 'B', 'C', 'P', '\\\\custom_abs_line', '\\\\custom_fraction_div', '\\\\sqrt{}', '\\\\theta', 'a', 'b', 'c', 'o', 'p', 'r', 's', 'x', 'y']\n",
      "-------------------CLASS NAMES-------------------\n",
      "Create YOLOv3 model with 9 anchors and 22 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((1, 1, 1024, 81) vs (255, 1024, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_59 due to mismatch in shape ((81,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((1, 1, 512, 81) vs (255, 512, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_67 due to mismatch in shape ((81,) vs (255,)).\n",
      "  weight_values[i].shape))\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((1, 1, 256, 81) vs (255, 256, 1, 1)).\n",
      "  weight_values[i].shape))\n",
      "c:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\saving.py:1140: UserWarning: Skipping loading of weights for layer conv2d_75 due to mismatch in shape ((81,) vs (255,)).\n",
      "  weight_values[i].shape))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load weights model_data/yolo.h5.\n",
      "Freeze the first 249 layers of total 252 layers.\n",
      "Train on 24 samples, val on 6 samples, with batch size 32.\n",
      "Epoch 1/100\n",
      "1/1 [==============================] - 15s 15s/step - loss: 5922.4307 - val_loss: 5662.5181\n",
      "Epoch 2/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 5434.9614 - val_loss: 5198.1787\n",
      "Epoch 3/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 4911.7168 - val_loss: 4629.6284\n",
      "Epoch 4/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 4417.7529 - val_loss: 4101.2407\n",
      "Epoch 5/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3949.7075 - val_loss: 3838.5410\n",
      "Epoch 6/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3519.7356 - val_loss: 3335.2026\n",
      "Epoch 7/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 3110.4910 - val_loss: 3021.3325\n",
      "Epoch 8/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2772.0122 - val_loss: 2675.9575\n",
      "Epoch 9/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2483.5925 - val_loss: 2414.4854\n",
      "Epoch 10/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 2202.8318 - val_loss: 2117.2446\n",
      "Epoch 11/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1955.2205 - val_loss: 1915.8129\n",
      "Epoch 12/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1749.7102 - val_loss: 1704.3710\n",
      "Epoch 13/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1579.3669 - val_loss: 1524.3344\n",
      "Epoch 14/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1395.0098 - val_loss: 1383.1212\n",
      "Epoch 15/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1277.0892 - val_loss: 1243.2686\n",
      "Epoch 16/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1158.8286 - val_loss: 1140.9719\n",
      "Epoch 17/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1035.8884 - val_loss: 1039.1245\n",
      "Epoch 18/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 949.0171 - val_loss: 942.3577\n",
      "Epoch 19/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 872.5251 - val_loss: 856.6273\n",
      "Epoch 20/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 796.8717 - val_loss: 796.5938\n",
      "Epoch 21/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 736.7350 - val_loss: 760.5867\n",
      "Epoch 22/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 683.8924 - val_loss: 673.7297\n",
      "Epoch 23/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 636.6365 - val_loss: 630.6600\n",
      "Epoch 24/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 592.5626 - val_loss: 601.4523\n",
      "Epoch 25/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 546.3467 - val_loss: 574.3695\n",
      "Epoch 26/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 523.3766 - val_loss: 526.7097\n",
      "Epoch 27/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 485.4966 - val_loss: 500.6652\n",
      "Epoch 28/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 478.1952 - val_loss: 469.5770\n",
      "Epoch 29/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 446.3159 - val_loss: 465.9136\n",
      "Epoch 30/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 430.4126 - val_loss: 441.2225\n",
      "Epoch 31/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 411.5372 - val_loss: 434.3994\n",
      "Epoch 32/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 398.6025 - val_loss: 411.5373\n",
      "Epoch 33/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 372.5005 - val_loss: 389.0058\n",
      "Epoch 34/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 366.6982 - val_loss: 374.8820\n",
      "Epoch 35/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 354.5081 - val_loss: 369.3745\n",
      "Epoch 36/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 343.6487 - val_loss: 355.8277\n",
      "Epoch 37/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 338.0225 - val_loss: 347.8158\n",
      "Epoch 38/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 327.3982 - val_loss: 356.1321\n",
      "Epoch 39/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 320.5987 - val_loss: 339.9281\n",
      "Epoch 40/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 306.7099 - val_loss: 326.4853\n",
      "Epoch 41/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 295.8116 - val_loss: 324.8896\n",
      "Epoch 42/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 304.9436 - val_loss: 322.4892\n",
      "Epoch 43/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 294.0145 - val_loss: 308.6468\n",
      "Epoch 44/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 288.5434 - val_loss: 304.1359\n",
      "Epoch 45/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 289.4429 - val_loss: 303.6347\n",
      "Epoch 46/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 285.5836 - val_loss: 300.0128\n",
      "Epoch 47/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 269.8856 - val_loss: 293.4090\n",
      "Epoch 48/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 265.7987 - val_loss: 292.0373\n",
      "Epoch 49/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 263.1109 - val_loss: 278.4162\n",
      "Epoch 50/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 263.6910 - val_loss: 277.3567\n",
      "Epoch 51/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 262.5325 - val_loss: 260.9231\n",
      "Epoch 52/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 263.8302 - val_loss: 273.9823\n",
      "Epoch 53/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 247.3599 - val_loss: 263.8777\n",
      "Epoch 54/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 249.3882 - val_loss: 264.3254\n",
      "Epoch 55/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 247.1824 - val_loss: 268.6146\n",
      "Epoch 56/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 238.8916 - val_loss: 258.3133\n",
      "Epoch 57/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 239.8210 - val_loss: 262.9110\n",
      "Epoch 58/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 236.5850 - val_loss: 257.3766\n",
      "Epoch 59/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 233.3080 - val_loss: 257.0460\n",
      "Epoch 60/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 243.6842 - val_loss: 247.7603\n",
      "Epoch 61/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 235.0658 - val_loss: 254.1782\n",
      "Epoch 62/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 224.4873 - val_loss: 251.9763\n",
      "Epoch 63/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 227.8988 - val_loss: 242.2743\n",
      "Epoch 64/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 232.0120 - val_loss: 249.6585\n",
      "Epoch 65/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 217.8231 - val_loss: 246.9833\n",
      "Epoch 66/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 236.5747 - val_loss: 234.7256\n",
      "Epoch 67/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 213.9146 - val_loss: 238.1769\n",
      "Epoch 68/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 220.7617 - val_loss: 239.9979\n",
      "Epoch 69/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 224.7883 - val_loss: 235.4476\n",
      "Epoch 70/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 229.8768 - val_loss: 234.6398\n",
      "Epoch 71/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 220.3419 - val_loss: 235.2104\n",
      "Epoch 72/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 220.3763 - val_loss: 225.7717\n",
      "Epoch 73/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 214.9832 - val_loss: 232.6480\n",
      "Epoch 74/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 210.6386 - val_loss: 223.9292\n",
      "Epoch 75/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 214.6906 - val_loss: 232.3270\n",
      "Epoch 76/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 211.9283 - val_loss: 223.8362\n",
      "Epoch 77/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 206.2482 - val_loss: 225.4047\n",
      "Epoch 78/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 216.6423 - val_loss: 235.0257\n",
      "Epoch 79/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 206.5324 - val_loss: 229.8940\n",
      "Epoch 80/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 205.2613 - val_loss: 222.9439\n",
      "Epoch 81/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 212.4347 - val_loss: 227.6193\n",
      "Epoch 82/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 209.5553 - val_loss: 218.8451\n",
      "Epoch 83/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 200.4259 - val_loss: 218.4956\n",
      "Epoch 84/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 196.4815 - val_loss: 215.7054\n",
      "Epoch 85/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 196.8140 - val_loss: 227.1259\n",
      "Epoch 86/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 202.1218 - val_loss: 222.3172\n",
      "Epoch 87/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 202.5903 - val_loss: 224.0562\n",
      "Epoch 88/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 197.3963 - val_loss: 215.9861\n",
      "Epoch 89/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 206.2360 - val_loss: 215.3545\n",
      "Epoch 90/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 196.7382 - val_loss: 218.2952\n",
      "Epoch 91/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 202.1348 - val_loss: 207.0675\n",
      "Epoch 92/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 199.4692 - val_loss: 206.4025\n",
      "Epoch 93/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 184.5970 - val_loss: 210.2883\n",
      "Epoch 94/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 196.1764 - val_loss: 216.3753\n",
      "Epoch 95/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 200.3733 - val_loss: 210.4097\n",
      "Epoch 96/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 184.2124 - val_loss: 210.7951\n",
      "Epoch 97/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 202.3752 - val_loss: 211.5060\n",
      "Epoch 98/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 178.0685 - val_loss: 216.4691\n",
      "Epoch 99/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 202.4782 - val_loss: 202.3177\n",
      "Epoch 100/100\n",
      "1/1 [==============================] - 2s 2s/step - loss: 183.3037 - val_loss: 210.8148\n",
      "Unfreeze all of the layers.\n",
      "Train on 24 samples, val on 6 samples, with batch size 32.\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[32,32,320,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node batch_normalization_1/FusedBatchNorm}} = FusedBatchNorm[T=DT_FLOAT, _class=[\"loc:@train...chNormGrad\"], data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_1/convolution, batch_normalization_1/gamma/read, batch_normalization_1/beta/read, batch_normalization_65/Const_4, batch_normalization_65/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node yolo_loss/while_2/strided_slice_1/stack_1/_5425}} = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_11657_yolo_loss/while_2/strided_slice_1/stack_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopyolo_loss/while_2/strided_slice_1/stack_2/_5213)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-196ba5f33202>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m     \u001b[0m_main\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-196ba5f33202>\u001b[0m in \u001b[0;36m_main\u001b[1;34m()\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m             callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n\u001b[0m\u001b[0;32m     84\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'trained_weights_final.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kasim\\.conda\\envs\\tf_gpu\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[32,32,320,320] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node batch_normalization_1/FusedBatchNorm}} = FusedBatchNorm[T=DT_FLOAT, _class=[\"loc:@train...chNormGrad\"], data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](conv2d_1/convolution, batch_normalization_1/gamma/read, batch_normalization_1/beta/read, batch_normalization_65/Const_4, batch_normalization_65/Const_4)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node yolo_loss/while_2/strided_slice_1/stack_1/_5425}} = _HostRecv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_11657_yolo_loss/while_2/strided_slice_1/stack_1\", tensor_type=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](^_cloopyolo_loss/while_2/strided_slice_1/stack_2/_5213)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\n",
    "from yolo3.utils import get_random_data\n",
    "\n",
    "\n",
    "def _main():\n",
    "    annotation_path = '_annotations.txt'  # path to Roboflow data annotations\n",
    "    log_dir = 'logs/000/'                 # where we're storing our logs\n",
    "    classes_path = '_classes.txt'         # path to Roboflow class names\n",
    "    anchors_path = 'model_data/yolo_anchors.txt'\n",
    "    class_names = get_classes(classes_path)\n",
    "    print(\"-------------------CLASS NAMES-------------------\")\n",
    "    print(class_names)\n",
    "    print(\"-------------------CLASS NAMES-------------------\")\n",
    "    num_classes = len(class_names)\n",
    "    anchors = get_anchors(anchors_path)\n",
    "\n",
    "    input_shape = (320,320) # multiple of 32, hw\n",
    "\n",
    "    is_tiny_version = len(anchors)==6 # default setting\n",
    "    if is_tiny_version:\n",
    "        model = create_tiny_model(input_shape, anchors, num_classes,\n",
    "            freeze_body=2, weights_path='model_data/tiny_yolo_weights.h5')\n",
    "    else:\n",
    "        model = create_model(input_shape, anchors, num_classes,\n",
    "            freeze_body=2, weights_path='model_data/yolo.h5') # make sure you know what you freeze\n",
    "\n",
    "    logging = TensorBoard(log_dir=log_dir)\n",
    "    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n",
    "        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n",
    "\n",
    "    val_split = 0.2 # set the size of the validation set\n",
    "    with open(annotation_path) as f:\n",
    "        lines = f.readlines()\n",
    "    np.random.seed(10101)\n",
    "    np.random.shuffle(lines)\n",
    "    np.random.seed(None)\n",
    "    num_val = int(len(lines)*val_split)\n",
    "    num_train = len(lines) - num_val\n",
    "\n",
    "    # Train with frozen layers first, to get a stable loss.\n",
    "    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n",
    "    if True:\n",
    "        model.compile(optimizer=Adam(lr=1e-3), loss={\n",
    "            # use custom yolo_loss Lambda layer.\n",
    "            'yolo_loss': lambda y_true, y_pred: y_pred})\n",
    "\n",
    "        batch_size = 32\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "                steps_per_epoch=max(1, num_train//batch_size),\n",
    "                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "                validation_steps=max(1, num_val//batch_size),\n",
    "                epochs=100,\n",
    "                initial_epoch=0,\n",
    "                callbacks=[logging, checkpoint])\n",
    "        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n",
    "\n",
    "    # Unfreeze and continue training, to fine-tune.\n",
    "    # Train longer if the result is not good.\n",
    "    if True:\n",
    "        for i in range(len(model.layers)):\n",
    "            model.layers[i].trainable = True\n",
    "        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n",
    "        print('Unfreeze all of the layers.')\n",
    "\n",
    "        batch_size = 32 # note that more GPU memory is required after unfreezing the body\n",
    "        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n",
    "        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n",
    "            steps_per_epoch=max(1, num_train//batch_size),\n",
    "            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n",
    "            validation_steps=max(1, num_val//batch_size),\n",
    "            epochs=100,\n",
    "            initial_epoch=50,\n",
    "            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n",
    "        model.save_weights(log_dir + 'trained_weights_final.h5')\n",
    "\n",
    "    # Further training if needed.\n",
    "\n",
    "\n",
    "def get_classes(classes_path):\n",
    "    '''loads the classes'''\n",
    "    with open(classes_path) as f:\n",
    "        class_names = f.readlines()\n",
    "    class_names = [c.strip() for c in class_names]\n",
    "    return class_names\n",
    "\n",
    "def get_anchors(anchors_path):\n",
    "    '''loads the anchors from a file'''\n",
    "    with open(anchors_path) as f:\n",
    "        anchors = f.readline()\n",
    "    anchors = [float(x) for x in anchors.split(',')]\n",
    "    return np.array(anchors).reshape(-1, 2)\n",
    "\n",
    "\n",
    "def create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/yolo.h5'):\n",
    "    '''create the training model'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16, 2:8}[l], w//{0:32, 1:16, 2:8}[l], \\\n",
    "        num_anchors//3, num_classes+5)) for l in range(3)]\n",
    "\n",
    "    model_body = yolo_body(image_input, num_anchors//3, num_classes)\n",
    "    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze darknet53 body or freeze all but 3 output layers.\n",
    "            num = (185, len(model_body.layers)-3)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.5})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_tiny_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n",
    "            weights_path='model_data/tiny_yolo_weights.h5'):\n",
    "    '''create the training model, for Tiny YOLOv3'''\n",
    "    K.clear_session() # get a new session\n",
    "    image_input = Input(shape=(None, None, 3))\n",
    "    h, w = input_shape\n",
    "    num_anchors = len(anchors)\n",
    "\n",
    "    y_true = [Input(shape=(h//{0:32, 1:16}[l], w//{0:32, 1:16}[l], \\\n",
    "        num_anchors//2, num_classes+5)) for l in range(2)]\n",
    "\n",
    "    model_body = tiny_yolo_body(image_input, num_anchors//2, num_classes)\n",
    "    print('Create Tiny YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n",
    "\n",
    "    if load_pretrained:\n",
    "        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
    "        print('Load weights {}.'.format(weights_path))\n",
    "        if freeze_body in [1, 2]:\n",
    "            # Freeze the darknet body or freeze all but 2 output layers.\n",
    "            num = (20, len(model_body.layers)-2)[freeze_body-1]\n",
    "            for i in range(num): model_body.layers[i].trainable = False\n",
    "            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n",
    "\n",
    "    model_loss = Lambda(yolo_loss, output_shape=(1,), name='yolo_loss',\n",
    "        arguments={'anchors': anchors, 'num_classes': num_classes, 'ignore_thresh': 0.7})(\n",
    "        [*model_body.output, *y_true])\n",
    "    model = Model([model_body.input, *y_true], model_loss)\n",
    "\n",
    "    return model\n",
    "\n",
    "def data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    '''data generator for fit_generator'''\n",
    "    n = len(annotation_lines)\n",
    "    i = 0\n",
    "    while True:\n",
    "        image_data = []\n",
    "        box_data = []\n",
    "        for b in range(batch_size):\n",
    "            if i==0:\n",
    "                np.random.shuffle(annotation_lines)\n",
    "            image, box = get_random_data(annotation_lines[i], input_shape, random=True)\n",
    "            image_data.append(image)\n",
    "            box_data.append(box)\n",
    "            i = (i+1) % n\n",
    "        image_data = np.array(image_data)\n",
    "        box_data = np.array(box_data)\n",
    "        y_true = preprocess_true_boxes(box_data, input_shape, anchors, num_classes)\n",
    "        yield [image_data, *y_true], np.zeros(batch_size)\n",
    "\n",
    "def data_generator_wrapper(annotation_lines, batch_size, input_shape, anchors, num_classes):\n",
    "    n = len(annotation_lines)\n",
    "    if n==0 or batch_size<=0: return None\n",
    "    return data_generator(annotation_lines, batch_size, input_shape, anchors, num_classes)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
